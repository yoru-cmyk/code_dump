{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecd609c9-b33c-4867-8931-ed9c39cf6461",
   "metadata": {},
   "source": [
    "Here we just check for class 16.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee708d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "#from torchvision import datasets\n",
    "#from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa81e3f",
   "metadata": {},
   "source": [
    "#https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f1215a",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c28a6",
   "metadata": {},
   "source": [
    "https://medium.com/@shashikachamod4u/excel-csv-to-pytorch-dataset-def496b6bcc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c651846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "csv = pd.read_csv(\"class16.csv\")\n",
    "csv = shuffle(csv)\n",
    "\n",
    "pre_full = csv\n",
    "pre_train=csv[0:2500]\n",
    "pre_test=csv[2500:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2065e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class csvDataset(Dataset):\n",
    "    def __init__(self,file):\n",
    "        file_out = file #pd.read_csv(file)\n",
    "        x=file_out.iloc[:,1:6].values\n",
    "        y=file_out.iloc[:,6:7].values\n",
    "\n",
    "        sc = StandardScaler()\n",
    "        x_train = sc.fit_transform(x)\n",
    "        y_train = y\n",
    "        \n",
    "        self.X_train = torch.tensor(x_train,dtype = torch.float32)\n",
    "        self.y_train = torch.tensor(y_train)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.X_train[idx].float(),self.y_train[idx].float()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68aa2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = csvDataset(pre_full)\n",
    "train = csvDataset(pre_train)\n",
    "test = csvDataset(pre_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a1a2572",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =500\n",
    "full_dataloader = DataLoader(full, batch_size=batch_size)\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc1d9b6",
   "metadata": {},
   "source": [
    "If you want to load a model use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32ea4189",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model = NeuralNetwork()\n",
    "#model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137b44c",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d7bc9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=11, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=11, out_features=2, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(5, 11),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(11, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1ad84c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=12, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=12, out_features=2, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(5, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8358506a",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "\n",
    "\n",
    "https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants\n",
    "\n",
    "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
    "\n",
    "https://analyticsindiamag.com/ultimate-guide-to-pytorch-optimizers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "08d8df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False) #torch.optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "323b1173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98bc5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    print(size)\n",
    "    num_batches = len(dataloader)\n",
    "    #model.eval() will notify all your layers that you are in eval mode\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    #torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you wonâ€™t be able to backprop\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            #print(f\"this is y{y}\")\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            #print(f\"this is pred:{pred}\")\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            #print(f\"this is predargmax{pred.argmax(1)}\")\n",
    "            \n",
    "            final_pred = pred.argmax(1)            \n",
    "\n",
    "            \n",
    "            #correct += (pred.argmax(1) == y.flatten()).type(torch.float).sum().item()\n",
    "            #print(torch.eq(final_pred,y.flatten()))\n",
    "            correct += torch.eq(final_pred,y.flatten()).sum().item()\n",
    "\n",
    "            #print(f\"this is condition{(pred.argmax(1) == y)}\")\n",
    "            #print(f\"This is correct:{correct}\")\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847117c0",
   "metadata": {},
   "source": [
    "https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c0a2fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for g in optimizer.param_groups:\n",
    "#    print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac6a7006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.000019  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000312 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.000020  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000306 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.000013  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000306 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 11.2%, Avg loss: 0.000307 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.000013  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000309 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.000013  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 5.2%, Avg loss: 0.000305 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000306 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.000306 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000313 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.000014  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.000306 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.000308 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.000309 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.7%, Avg loss: 0.000311 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 2.0%, Avg loss: 0.000321 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.000021  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000317 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.000018  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000319 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.000020  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 9.4%, Avg loss: 0.000346 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.000046  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 7.9%, Avg loss: 0.000318 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.000016  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 3.4%, Avg loss: 0.000359 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.000066  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 6.1%, Avg loss: 0.000313 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 11.9%, Avg loss: 0.000323 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.000022  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000343 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.000044  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.000314 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.000013  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000317 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.000016  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 81.0%, Avg loss: 0.000314 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000319 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.000017  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000315 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000321 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.000017  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 32.5%, Avg loss: 0.000316 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000321 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.000016  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.1%, Avg loss: 0.000316 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.000321 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.000013  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.000321 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.000015  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.000319 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.000319 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 0.000319 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000323 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.000015  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000321 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.000323 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.000013  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000327 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.000018  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000334 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.000025  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000330 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.000019  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.000323 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.000322 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.000323 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000327 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.000013  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000328 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.000015  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.000329 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.000017  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.1%, Avg loss: 0.000324 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000325 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000342 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.000027  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 51.6%, Avg loss: 0.000339 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.000025  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 6.4%, Avg loss: 0.000328 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 42.8%, Avg loss: 0.000325 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000329 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.000013  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000343 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.000026  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.6%, Avg loss: 0.000335 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.000019  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000342 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.000023  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 30.1%, Avg loss: 0.000339 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.000021  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000342 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.000022  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.000343 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.000022  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.000331 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.000337 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.000017  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000333 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 5.0%, Avg loss: 0.000343 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.000021  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 28.6%, Avg loss: 0.000334 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 16.4%, Avg loss: 0.000336 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000335 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000333 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.7%, Avg loss: 0.000334 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 45.3%, Avg loss: 0.000336 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000340 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.000015  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 4.4%, Avg loss: 0.000340 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.000015  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.2%, Avg loss: 0.000336 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 17.3%, Avg loss: 0.000337 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 7.5%, Avg loss: 0.000336 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 5.0%, Avg loss: 0.000346 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.000019  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.000337 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.1%, Avg loss: 0.000343 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.000013  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 48.4%, Avg loss: 0.000336 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 89.1%, Avg loss: 0.000338 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.0%, Avg loss: 0.000340 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000340 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000340 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000341 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000345 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000342 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.1%, Avg loss: 0.000343 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000344 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 2.0%, Avg loss: 0.000342 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.1%, Avg loss: 0.000343 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000347 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 4.6%, Avg loss: 0.000344 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000359 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.000021  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000348 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.000376 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.000044  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000349 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000363 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.000026  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000371 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.000035  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000359 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.000018  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 40.9%, Avg loss: 0.000351 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000353 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.000365 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.000027  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.000351 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000352 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000351 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.000384 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.000047  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 18.7%, Avg loss: 0.000350 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000372 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.000030  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.000354 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000352 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 14.9%, Avg loss: 0.000362 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.000016  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000353 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 15.2%, Avg loss: 0.000354 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000361 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.000014  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000360 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 10.7%, Avg loss: 0.000355 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.2%, Avg loss: 0.000355 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.2%, Avg loss: 0.000357 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.000355 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000357 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000365 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.000015  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.000394 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.000045  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000364 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.000379 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.000031  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000379 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.000030  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000447 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.000099  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.000381 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.000028  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 0.000362 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 5.6%, Avg loss: 0.000378 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.000027  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000430 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.000080  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000366 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.3%, Avg loss: 0.000393 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.000040  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000370 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.000442 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.000090  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000395 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.000037  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000370 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.000013  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000414 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.000062  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000402 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.000047  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000390 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.000030  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 28.2%, Avg loss: 0.000395 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.000035  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000370 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000379 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.000018  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000369 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000370 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 7.1%, Avg loss: 0.000366 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.000371 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 24.5%, Avg loss: 0.000370 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000370 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 3.8%, Avg loss: 0.000369 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 25.3%, Avg loss: 0.000372 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 27.5%, Avg loss: 0.000372 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000373 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 36.6%, Avg loss: 0.000376 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 50.9%, Avg loss: 0.000382 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.000015  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 3.0%, Avg loss: 0.000374 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.1%, Avg loss: 0.000392 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.000025  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000387 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.000022  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.1%, Avg loss: 0.000373 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.2%, Avg loss: 0.000376 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.000378 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.000379 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000378 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000377 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000377 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.000378 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 19.5%, Avg loss: 0.000378 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 20.0%, Avg loss: 0.000377 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.000378 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 0.000378 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.000379 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.6%, Avg loss: 0.000381 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.4%, Avg loss: 0.000382 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 6.6%, Avg loss: 0.000381 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.0%, Avg loss: 0.000381 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 3.1%, Avg loss: 0.000384 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.1%, Avg loss: 0.000386 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000384 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 6.6%, Avg loss: 0.000383 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.1%, Avg loss: 0.000384 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000384 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.6%, Avg loss: 0.000384 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000387 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000391 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.000386 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000394 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.000014  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 22.1%, Avg loss: 0.000387 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000398 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.000015  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000389 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000387 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000393 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 2.8%, Avg loss: 0.000389 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000389 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 38.8%, Avg loss: 0.000390 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.7%, Avg loss: 0.000389 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.000389 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 0.000390 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.000391 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 2.9%, Avg loss: 0.000396 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 95.5%, Avg loss: 0.000394 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000397 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 4.7%, Avg loss: 0.000398 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000408 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.000020  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000404 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 0.000018  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000402 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 0.000015  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.2%, Avg loss: 0.000396 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000395 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000397 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.0%, Avg loss: 0.000394 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000398 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000403 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 0.000397 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 4.2%, Avg loss: 0.000396 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000398 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000405 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 15.7%, Avg loss: 0.000414 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.000024  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.000403 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000402 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000399 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.000399 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 0.000399 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 5.4%, Avg loss: 0.000407 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000401 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000401 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000401 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.1%, Avg loss: 0.000402 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000402 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.000401 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 91.0%, Avg loss: 0.000406 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000403 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 27.6%, Avg loss: 0.000400 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.2%, Avg loss: 0.000407 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 52.8%, Avg loss: 0.000429 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.000031  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 15.0%, Avg loss: 0.000404 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.8%, Avg loss: 0.000412 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.000016  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.000406 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 3.8%, Avg loss: 0.000404 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 95.3%, Avg loss: 0.000412 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.000015  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.000406 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 34.2%, Avg loss: 0.000404 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 70.6%, Avg loss: 0.000406 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 19.8%, Avg loss: 0.000406 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.000409 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 69.2%, Avg loss: 0.000408 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 13.6%, Avg loss: 0.000405 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 0.000408 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 0.000408 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.000425 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.000023  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 4.8%, Avg loss: 0.000408 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000420 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.000019  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000421 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.000017  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000412 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000409 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.1%, Avg loss: 0.000408 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000411 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000414 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000409 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.3%, Avg loss: 0.000412 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.000411 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 31.3%, Avg loss: 0.000411 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000411 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.000413 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.7%, Avg loss: 0.000412 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.6%, Avg loss: 0.000416 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000414 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.000413 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000416 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.000413 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.000412 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.4%, Avg loss: 0.000413 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000414 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000417 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 0.000414 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000415 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 0.000416 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 0.000418 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.8%, Avg loss: 0.000417 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.6%, Avg loss: 0.000415 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.000416 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 0.000417 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 48.3%, Avg loss: 0.000448 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 0.000035  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000419 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.000425 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.5%, Avg loss: 0.000442 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.000029  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 36.4%, Avg loss: 0.000434 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.000019  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000420 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 4.2%, Avg loss: 0.000427 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.000014  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000440 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.000024  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000422 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000447 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 0.000034  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.2%, Avg loss: 0.000442 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.000025  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000421 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000423 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.000442 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.000027  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.000429 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.000017  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000423 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000422 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 20.9%, Avg loss: 0.000426 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000423 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000424 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.1%, Avg loss: 0.000430 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 48.7%, Avg loss: 0.000425 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000424 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000424 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000447 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 0.000031  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 0.000422 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 7.3%, Avg loss: 0.000430 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000425 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.1%, Avg loss: 0.000424 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000427 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000427 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 46.7%, Avg loss: 0.000425 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 5.9%, Avg loss: 0.000426 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.000430 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 66.6%, Avg loss: 0.000431 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 0.000438 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 0.000016  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000426 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000442 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 0.000024  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000431 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000434 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 0.000013  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000436 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 0.000016  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000445 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 0.000025  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000439 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 0.000016  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000438 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 0.000014  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000431 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 2.0%, Avg loss: 0.000431 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 0.000439 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000428 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000435 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 2.0%, Avg loss: 0.000431 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.000430 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.5%, Avg loss: 0.000430 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 27.9%, Avg loss: 0.000432 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 47.4%, Avg loss: 0.000432 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 0.000002  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 43.6%, Avg loss: 0.000433 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.000431 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 9.4%, Avg loss: 0.000431 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 29.3%, Avg loss: 0.000432 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000432 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 16.5%, Avg loss: 0.000435 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.3%, Avg loss: 0.000433 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 27.8%, Avg loss: 0.000434 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.2%, Avg loss: 0.000434 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 50.7%, Avg loss: 0.000436 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 90.5%, Avg loss: 0.000438 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 0.000435 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000440 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 3.7%, Avg loss: 0.000441 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.3%, Avg loss: 0.000434 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 0.000002  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.000440 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000438 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000441 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000442 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 21.3%, Avg loss: 0.000437 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000437 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000437 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000438 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.7%, Avg loss: 0.000442 \n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 0.000452 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 0.000020  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000439 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000441 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000440 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 87.0%, Avg loss: 0.000441 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000444 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.000464 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 0.000031  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.000447 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.2%, Avg loss: 0.000452 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 0.000017  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000443 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.000485 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 0.000050  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 7.4%, Avg loss: 0.000444 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000447 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 0.000014  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000445 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000441 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000443 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000454 \n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "loss: 0.000024  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000446 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 2.6%, Avg loss: 0.000440 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "loss: 0.000002  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000448 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000446 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000439 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000442 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000440 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000440 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.000440 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "loss: 0.000002  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000445 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 9.4%, Avg loss: 0.000445 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000439 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 14.4%, Avg loss: 0.000442 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000441 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000442 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.7%, Avg loss: 0.000440 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "loss: 0.000002  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000445 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 0.000448 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000449 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "loss: 0.000013  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.000444 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000441 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000449 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000442 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000445 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000444 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000454 \n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000446 \n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.2%, Avg loss: 0.000447 \n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Avg loss: 0.000446 \n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Avg loss: 0.000458 \n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "loss: 0.000016  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000445 \n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000445 \n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000450 \n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000445 \n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 2.6%, Avg loss: 0.000446 \n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000446 \n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.4%, Avg loss: 0.000444 \n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000447 \n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 4.3%, Avg loss: 0.000458 \n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "loss: 0.000015  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000452 \n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "loss: 0.000013  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000449 \n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000446 \n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.000447 \n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000447 \n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000445 \n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.000446 \n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 14.4%, Avg loss: 0.000454 \n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "loss: 0.000013  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000449 \n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000446 \n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 66.3%, Avg loss: 0.000447 \n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000452 \n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000450 \n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000445 \n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 9.2%, Avg loss: 0.000443 \n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "loss: 0.000002  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.2%, Avg loss: 0.000445 \n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000455 \n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 10.8%, Avg loss: 0.000448 \n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000447 \n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000457 \n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "loss: 0.000014  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 0.000454 \n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.7%, Avg loss: 0.000456 \n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "loss: 0.000015  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.1%, Avg loss: 0.000459 \n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "loss: 0.000016  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.000450 \n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "loss: 0.000010  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.2%, Avg loss: 0.000448 \n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000448 \n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000467 \n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "loss: 0.000023  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 27.0%, Avg loss: 0.000448 \n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 38.6%, Avg loss: 0.000447 \n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 15.6%, Avg loss: 0.000449 \n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000448 \n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.000450 \n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000448 \n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000446 \n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.0%, Avg loss: 0.000453 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000475 \n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "loss: 0.000032  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 95.3%, Avg loss: 0.000450 \n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.000481 \n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "loss: 0.000042  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.1%, Avg loss: 0.000455 \n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "loss: 0.000011  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.2%, Avg loss: 0.000498 \n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "loss: 0.000056  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.000449 \n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000458 \n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "loss: 0.000015  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000457 \n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "loss: 0.000014  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000463 \n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "loss: 0.000020  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.000449 \n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.000448 \n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "loss: 0.000002  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.000450 \n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000468 \n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "loss: 0.000021  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000468 \n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "loss: 0.000026  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000471 \n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "loss: 0.000026  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000450 \n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.000461 \n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "loss: 0.000016  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000456 \n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "loss: 0.000013  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000452 \n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 0.000450 \n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000452 \n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000460 \n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "loss: 0.000015  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000451 \n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000458 \n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000458 \n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "loss: 0.000012  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 0.000453 \n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 46.3%, Avg loss: 0.000462 \n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "loss: 0.000017  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000449 \n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "loss: 0.000003  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 2.5%, Avg loss: 0.000453 \n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.2%, Avg loss: 0.000476 \n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "loss: 0.000036  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000495 \n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "loss: 0.000051  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 0.000447 \n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "loss: 0.000002  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000466 \n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "loss: 0.000024  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000453 \n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000453 \n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "loss: 0.000007  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000484 \n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "loss: 0.000044  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 34.5%, Avg loss: 0.000456 \n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "loss: 0.000009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000452 \n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg loss: 0.000462 \n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "loss: 0.000015  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.000452 \n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "loss: 0.000005  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000467 \n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "loss: 0.000020  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 0.000454 \n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "loss: 0.000008  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.000473 \n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "loss: 0.000026  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.4%, Avg loss: 0.000464 \n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "loss: 0.000018  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000462 \n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "loss: 0.000015  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000463 \n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "loss: 0.000019  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000464 \n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "loss: 0.000020  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.000451 \n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "loss: 0.000006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000452 \n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.3%, Avg loss: 0.000452 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "loss: 0.000004  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 1.0%, Avg loss: 0.000470 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcd20c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "#If you want to save the model. \n",
    "#torch.save(model.state_dict(), \"Vanilla SGD model.pth\")\n",
    "#print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8337f1-8eed-4830-a236-4dbf6091fdf0",
   "metadata": {},
   "source": [
    "Not that much better than class 77 as the accuracy jumps all over the place , BUT at least the average loss decreases. \n",
    "\n",
    "it suggest other problem, e.g. not enough data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6646e227-050d-4921-8604-f437e4d75211",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41b4e1f-6837-41af-914e-0f0eacc953ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
