{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06956736-e8e8-441f-9306-f9a08d1ce933",
   "metadata": {},
   "source": [
    "Ok so now I have tried PCA with X results. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a34c5d2-264e-4d3e-9fbd-0b8b60b79b40",
   "metadata": {},
   "source": [
    "There are several things I did try. \n",
    "The first was feature engineering. \n",
    "\n",
    "I read this paper: \n",
    "http://www.deepscn.com/pdfs/Pao%20and%20Takefji_1992.pdf\n",
    "\n",
    "In a nutshell, it says that if you add more features derived from other features.\n",
    "If you read it, it's a bit of a mystery how it works. But let's give it a shot and see what it does. \n",
    "\n",
    "Normally, I'd also  would based on domain knowledge\n",
    "(for example, if you're trying to predict power usage, you might create another feature based on temperature and humidity), but in this case I didn't really have a lot of context for the dataset beyond \"this is an anti-fraud dataset\" so I decided to just used the tan function to help me.\n",
    "\n",
    "To keep it simple and easier to do compare, I also used the same architecture as prior with only the input layer changed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee708d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import OrderedDict\n",
    "\n",
    "import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c651846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "#https://medium.com/@shashikachamod4u/excel-csv-to-pytorch-dataset-def496b6bcc1\n",
    "#Learning rate modifier downstairs\n",
    "\n",
    "#https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html\n",
    "\n",
    "\n",
    "#seed 0 is a good one!\n",
    "#https://stackoverflow.com/questions/19306976/python-shuffling-with-a-parameter-to-get-the-same-result\n",
    "\n",
    "\n",
    "csv = pd.read_csv(\"class 77 synth.csv\", sep=';' )\n",
    "csv = shuffle(csv)\n",
    "\n",
    "pre_full = csv\n",
    "pre_train=csv[0:2500]\n",
    "pre_test=csv[2500:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "225b2011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>TANX2</th>\n",
       "      <th>TanX3</th>\n",
       "      <th>Tanx4</th>\n",
       "      <th>TanX5</th>\n",
       "      <th>TanX6</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>-0.936030</td>\n",
       "      <td>0.855527</td>\n",
       "      <td>0.006320</td>\n",
       "      <td>-0.560525</td>\n",
       "      <td>0.044400</td>\n",
       "      <td>-1.357883</td>\n",
       "      <td>1.151101</td>\n",
       "      <td>0.006320</td>\n",
       "      <td>-0.627681</td>\n",
       "      <td>0.044429</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>0.490733</td>\n",
       "      <td>0.347759</td>\n",
       "      <td>0.165601</td>\n",
       "      <td>0.687637</td>\n",
       "      <td>0.163640</td>\n",
       "      <td>0.534330</td>\n",
       "      <td>0.362490</td>\n",
       "      <td>0.167131</td>\n",
       "      <td>0.821372</td>\n",
       "      <td>0.165117</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>0.608384</td>\n",
       "      <td>0.270552</td>\n",
       "      <td>-0.356786</td>\n",
       "      <td>-0.830551</td>\n",
       "      <td>-0.671067</td>\n",
       "      <td>0.696516</td>\n",
       "      <td>0.277353</td>\n",
       "      <td>-0.372738</td>\n",
       "      <td>-1.094644</td>\n",
       "      <td>-0.793993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3558</th>\n",
       "      <td>-0.098200</td>\n",
       "      <td>-0.597729</td>\n",
       "      <td>-0.407794</td>\n",
       "      <td>-0.692466</td>\n",
       "      <td>-0.086400</td>\n",
       "      <td>-0.098517</td>\n",
       "      <td>-0.680808</td>\n",
       "      <td>-0.432011</td>\n",
       "      <td>-0.829491</td>\n",
       "      <td>-0.086616</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2636</th>\n",
       "      <td>0.511986</td>\n",
       "      <td>0.107689</td>\n",
       "      <td>0.336475</td>\n",
       "      <td>0.409854</td>\n",
       "      <td>0.092500</td>\n",
       "      <td>0.561969</td>\n",
       "      <td>0.108108</td>\n",
       "      <td>0.349775</td>\n",
       "      <td>0.434458</td>\n",
       "      <td>0.092765</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>0.074900</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.189390</td>\n",
       "      <td>0.137721</td>\n",
       "      <td>0.656542</td>\n",
       "      <td>0.075040</td>\n",
       "      <td>0.018702</td>\n",
       "      <td>0.191687</td>\n",
       "      <td>0.138599</td>\n",
       "      <td>0.770579</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.395661</td>\n",
       "      <td>-0.718361</td>\n",
       "      <td>-0.299357</td>\n",
       "      <td>-0.888328</td>\n",
       "      <td>0.629872</td>\n",
       "      <td>0.417688</td>\n",
       "      <td>-0.874172</td>\n",
       "      <td>-0.308632</td>\n",
       "      <td>-1.230388</td>\n",
       "      <td>0.728919</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4901</th>\n",
       "      <td>0.670007</td>\n",
       "      <td>-0.170011</td>\n",
       "      <td>-0.730044</td>\n",
       "      <td>0.465083</td>\n",
       "      <td>-0.245571</td>\n",
       "      <td>0.792266</td>\n",
       "      <td>-0.171668</td>\n",
       "      <td>-0.894998</td>\n",
       "      <td>0.501796</td>\n",
       "      <td>-0.250630</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>-0.198517</td>\n",
       "      <td>0.591385</td>\n",
       "      <td>0.450856</td>\n",
       "      <td>-0.198618</td>\n",
       "      <td>0.294775</td>\n",
       "      <td>-0.201167</td>\n",
       "      <td>0.671563</td>\n",
       "      <td>0.484111</td>\n",
       "      <td>-0.201271</td>\n",
       "      <td>0.303620</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.631194</td>\n",
       "      <td>0.854685</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.812299</td>\n",
       "      <td>-0.859999</td>\n",
       "      <td>0.730945</td>\n",
       "      <td>1.149146</td>\n",
       "      <td>0.070718</td>\n",
       "      <td>1.055303</td>\n",
       "      <td>-1.161553</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X2        X3        X4        X5        X6     TANX2     TanX3  \\\n",
       "2098 -0.936030  0.855527  0.006320 -0.560525  0.044400 -1.357883  1.151101   \n",
       "3189  0.490733  0.347759  0.165601  0.687637  0.163640  0.534330  0.362490   \n",
       "1118  0.608384  0.270552 -0.356786 -0.830551 -0.671067  0.696516  0.277353   \n",
       "3558 -0.098200 -0.597729 -0.407794 -0.692466 -0.086400 -0.098517 -0.680808   \n",
       "2636  0.511986  0.107689  0.336475  0.409854  0.092500  0.561969  0.108108   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2260  0.074900  0.018700  0.189390  0.137721  0.656542  0.075040  0.018702   \n",
       "1995  0.395661 -0.718361 -0.299357 -0.888328  0.629872  0.417688 -0.874172   \n",
       "4901  0.670007 -0.170011 -0.730044  0.465083 -0.245571  0.792266 -0.171668   \n",
       "2022 -0.198517  0.591385  0.450856 -0.198618  0.294775 -0.201167  0.671563   \n",
       "11    0.631194  0.854685  0.070600  0.812299 -0.859999  0.730945  1.149146   \n",
       "\n",
       "         Tanx4     TanX5     TanX6  class  \n",
       "2098  0.006320 -0.627681  0.044429      1  \n",
       "3189  0.167131  0.821372  0.165117      1  \n",
       "1118 -0.372738 -1.094644 -0.793993      1  \n",
       "3558 -0.432011 -0.829491 -0.086616      1  \n",
       "2636  0.349775  0.434458  0.092765      1  \n",
       "...        ...       ...       ...    ...  \n",
       "2260  0.191687  0.138599  0.770579      1  \n",
       "1995 -0.308632 -1.230388  0.728919      1  \n",
       "4901 -0.894998  0.501796 -0.250630      1  \n",
       "2022  0.484111 -0.201271  0.303620      1  \n",
       "11    0.070718  1.055303 -1.161553      1  \n",
       "\n",
       "[5000 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ca24d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn CSV panda into numpy array\n",
    "csv_np = csv.values\n",
    "pre_train_np = pre_train.values\n",
    "pre_test_np = pre_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2065e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class csvDataset(Dataset):\n",
    "    def __init__(self,file):\n",
    "        file_out = file #pd.read_csv(file)\n",
    "        x=file_out.iloc[:,0:-1].values\n",
    "        y=file_out.iloc[:,-1:].values\n",
    "\n",
    "        sc = StandardScaler()\n",
    "        x_train = sc.fit_transform(x)\n",
    "        y_train = y\n",
    "        \n",
    "        self.X_train = torch.tensor(x_train,dtype = torch.float32)\n",
    "        self.y_train = torch.tensor(y_train)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.X_train[idx].float(),self.y_train[idx].float()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68aa2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = csvDataset(pre_full)\n",
    "train = csvDataset(pre_train)\n",
    "test = csvDataset(pre_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a1a2572",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =500\n",
    "full_dataloader = DataLoader(full, batch_size=batch_size)\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc1d9b6",
   "metadata": {},
   "source": [
    "If you want to load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137b44c",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1ad84c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=12, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=12, out_features=18, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=18, out_features=2, bias=True)\n",
      "    (5): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(10, 12),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(12, 18),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(18, 2),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "#https://discuss.pytorch.org/t/reset-model-weights/19180\n",
    "# Neural network model |---> Void\n",
    "#Usage : model.apply(weight_init)\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform(m.weight.data)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8358506a",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "\n",
    "\n",
    "https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants\n",
    "\n",
    "https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n",
    "\n",
    "https://analyticsindiamag.com/ultimate-guide-to-pytorch-optimizers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e098bcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/56435961/how-to-access-the-network-weights-while-using-pytorch-nn-sequential\n",
    "#model.linear_relu_stack[0].weight = nn.Parameter(model.linear_relu_stack[0].weight*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08d8df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False) \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "323b1173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98bc5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the testing function: \n",
    "# it tests for  and \n",
    "# returns accuracy\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    print(size)\n",
    "    num_batches = len(dataloader)\n",
    "    #model.eval() will notify all your layers that you are in eval mode\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    #torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "                \n",
    "            final_pred = pred.argmax(1)            \n",
    "            correct += torch.eq(final_pred,y.flatten()).sum().item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847117c0",
   "metadata": {},
   "source": [
    "https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5971632e",
   "metadata": {},
   "source": [
    "Learning rate modifier:\n",
    "\n",
    "https://stackoverflow.com/questions/48324152/pytorch-how-to-change-the-learning-rate-of-an-optimizer-at-any-given-moment-no"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12387c28-786c-47f8-b58f-5a21bad83e1b",
   "metadata": {},
   "source": [
    "I did one extra thing here: \n",
    "    \n",
    "I changed the learning rate so that it would decrease exponentially fast based on whether or not the last iteration made any improvement up to a certain amount of steps before resetting. \n",
    "\n",
    "It works as an exploration bonus.The big idea here is to be able to \"leap\" through local minima. \n",
    "Think of it as a soldier sweeping a minefield with his metal detector before taking a step in the safest direction.\n",
    "Repeat ad nauseum or until safety is reached.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "612af09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_epoch = 500\n",
    "\n",
    "#Usage:\n",
    "#n_epoch=number of epoch, start with 0\n",
    "#end_epoch = at what time do you want to end the epoch? \n",
    "#previous = the best score (in % ) up to date\n",
    "#step_size\n",
    "\n",
    "def test_train(n_epoch, end_epoch, previous_best,step_size_idx,very_best,best_model):\n",
    "    print(f\"Epoch {n_epoch}\\n-------------------------------\")\n",
    "    print(f\"Previous best:{previous_best}\")\n",
    "    print(f\"step size index:{step_size_idx}\")\n",
    "    \n",
    "    lr = 1/2**(step_size_idx%30)\n",
    "    print(f\"learning rate: {lr}\")\n",
    "#    optimizer = torch.optim.Adam(model.parameters(), lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr\n",
    "\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    score = test(test_dataloader, model, loss_fn)\n",
    "    \n",
    "    if very_best < score:\n",
    "        best_model = copy.deepcopy(model)\n",
    "        very_best = score\n",
    "\n",
    "    if score <= previous_best:\n",
    "        step_size_idx += 1 \n",
    "        best = score\n",
    "    else:\n",
    "        best = score        \n",
    "    \n",
    "    if n_epoch >= end_epoch:\n",
    "        print(\"done!\")\n",
    "        return best_model\n",
    "    else:\n",
    "        return test_train(n_epoch+1,end_epoch, best, step_size_idx,very_best,best_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e432d1-77b2-4e69-802e-c049f6bdc711",
   "metadata": {},
   "source": [
    "Lets take this baby for a test drive: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0676a4d5-6580-471f-91cc-f20acb18652a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "Previous best:0\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.945484  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 16.7%, Avg loss: 0.015076 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Previous best:0.1672\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.014253  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.1%, Avg loss: 0.012355 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Previous best:0.2612\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.011512  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 33.9%, Avg loss: 0.011174 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Previous best:0.3388\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.010321  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 41.1%, Avg loss: 0.010519 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Previous best:0.4108\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.009659  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 44.6%, Avg loss: 0.010104 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Previous best:0.4456\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.009239  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 46.8%, Avg loss: 0.009818 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Previous best:0.4684\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.008949  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 48.8%, Avg loss: 0.009611 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Previous best:0.4876\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.008737  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 49.9%, Avg loss: 0.009452 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Previous best:0.4992\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.008574  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 51.8%, Avg loss: 0.009327 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Previous best:0.5176\n",
      "step size index:0\n",
      "learning rate: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flackjacket/miniconda3/envs/machine-learning-env/lib/python3.9/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([500, 1])) that is different to the input size (torch.Size([500, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.008444  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 53.2%, Avg loss: 0.009225 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Previous best:0.5324\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.008338  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 0.009140 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Previous best:0.546\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.008249  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.009068 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Previous best:0.5548\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.008172  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 56.4%, Avg loss: 0.009006 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Previous best:0.5636\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.008104  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 57.3%, Avg loss: 0.008951 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Previous best:0.5728\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.008043  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Avg loss: 0.008901 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Previous best:0.5788\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.007988  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Avg loss: 0.008856 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Previous best:0.5876\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.007936  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Avg loss: 0.008814 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Previous best:0.6008\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.007888  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 0.008776 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Previous best:0.6024\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.007842  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 0.008739 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Previous best:0.6044\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.007798  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 0.008704 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Previous best:0.6048\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.007755  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.008671 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Previous best:0.606\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.007713  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 0.008639 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Previous best:0.6068\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.007672  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008608 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Previous best:0.6076\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.007631  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008577 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Previous best:0.6076\n",
      "step size index:1\n",
      "learning rate: 0.5\n",
      "loss: 0.007590  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Avg loss: 0.008562 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Previous best:0.6068\n",
      "step size index:2\n",
      "learning rate: 0.25\n",
      "loss: 0.007569  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008554 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Previous best:0.6076\n",
      "step size index:2\n",
      "learning rate: 0.25\n",
      "loss: 0.007558  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Avg loss: 0.008546 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Previous best:0.606\n",
      "step size index:3\n",
      "learning rate: 0.125\n",
      "loss: 0.007548  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Avg loss: 0.008543 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Previous best:0.6092\n",
      "step size index:3\n",
      "learning rate: 0.125\n",
      "loss: 0.007543  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008539 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Previous best:0.6084\n",
      "step size index:4\n",
      "learning rate: 0.0625\n",
      "loss: 0.007537  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008537 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:5\n",
      "learning rate: 0.03125\n",
      "loss: 0.007535  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008536 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Previous best:0.6084\n",
      "step size index:5\n",
      "learning rate: 0.03125\n",
      "loss: 0.007533  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008535 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Previous best:0.6076\n",
      "step size index:6\n",
      "learning rate: 0.015625\n",
      "loss: 0.007532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008535 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Previous best:0.6084\n",
      "step size index:6\n",
      "learning rate: 0.015625\n",
      "loss: 0.007531  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Previous best:0.6084\n",
      "step size index:7\n",
      "learning rate: 0.0078125\n",
      "loss: 0.007531  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Previous best:0.6084\n",
      "step size index:8\n",
      "learning rate: 0.00390625\n",
      "loss: 0.007531  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Previous best:0.6084\n",
      "step size index:9\n",
      "learning rate: 0.001953125\n",
      "loss: 0.007531  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:10\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:11\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:12\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:13\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:14\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:15\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:16\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:17\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:18\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:19\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:20\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:21\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:22\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:23\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:24\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:25\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:26\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:27\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:28\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:29\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 0.008534 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Previous best:0.608\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.007530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Avg loss: 0.008504 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Previous best:0.6112\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.007488  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 0.008475 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Previous best:0.6128\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.007447  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Avg loss: 0.008445 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Previous best:0.6188\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.007405  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 0.008416 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Previous best:0.6236\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.007362  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Avg loss: 0.008388 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Previous best:0.6256\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.007320  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.008359 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Previous best:0.6276\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.007278  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Avg loss: 0.008331 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Previous best:0.6356\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.007236  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 0.008302 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Previous best:0.6404\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.007194  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.008274 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Previous best:0.644\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.007153  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 65.2%, Avg loss: 0.008246 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Previous best:0.652\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.007111  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.008217 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Previous best:0.6532\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.007069  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 0.008188 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Previous best:0.658\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.007028  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 66.4%, Avg loss: 0.008157 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Previous best:0.664\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.006986  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 67.5%, Avg loss: 0.008127 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Previous best:0.6748\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.006944  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 0.008096 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Previous best:0.6856\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.006901  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 69.9%, Avg loss: 0.008062 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Previous best:0.6992\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.006857  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.008028 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Previous best:0.716\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.006811  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.007993 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Previous best:0.7292\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.006764  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 73.5%, Avg loss: 0.007957 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Previous best:0.7348\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.006716  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 74.4%, Avg loss: 0.007919 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Previous best:0.7444\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.006667  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.007880 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Previous best:0.7516\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.006615  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.007840 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Previous best:0.7584\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.006561  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 76.4%, Avg loss: 0.007799 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Previous best:0.7644\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.006505  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 76.9%, Avg loss: 0.007756 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Previous best:0.7692\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.006449  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 77.4%, Avg loss: 0.007712 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Previous best:0.774\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.006389  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 77.6%, Avg loss: 0.007666 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Previous best:0.7764\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.006327  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 0.007619 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Previous best:0.7788\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.006264  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 0.007572 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Previous best:0.7804\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.006199  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.4%, Avg loss: 0.007521 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Previous best:0.784\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.006132  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.8%, Avg loss: 0.007471 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Previous best:0.7876\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.006062  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 79.1%, Avg loss: 0.007418 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Previous best:0.7908\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.005993  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.007363 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Previous best:0.792\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.005922  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.007308 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Previous best:0.7936\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.005848  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 79.6%, Avg loss: 0.007250 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Previous best:0.796\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.005776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.007191 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Previous best:0.7984\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.005703  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 0.007130 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Previous best:0.802\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.005626  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.007067 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Previous best:0.8052\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.005550  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.007002 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Previous best:0.8064\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.005475  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 80.9%, Avg loss: 0.006936 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Previous best:0.8088\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.005401  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 81.1%, Avg loss: 0.006869 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Previous best:0.8108\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.005326  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 81.2%, Avg loss: 0.006800 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Previous best:0.8124\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.005254  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.006728 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Previous best:0.8148\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.005182  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 81.8%, Avg loss: 0.006657 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Previous best:0.818\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.005113  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.006584 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Previous best:0.8224\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.005040  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.006510 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Previous best:0.8248\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.004972  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.006435 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Previous best:0.8276\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.004907  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.006359 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.004843  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.006284 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Previous best:0.8308\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.004778  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.006208 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Previous best:0.8312\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.004718  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.006132 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Previous best:0.832\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.004663  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.006055 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Previous best:0.832\n",
      "step size index:31\n",
      "learning rate: 0.5\n",
      "loss: 0.004603  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.006006 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Previous best:0.8328\n",
      "step size index:31\n",
      "learning rate: 0.5\n",
      "loss: 0.004531  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005965 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Previous best:0.832\n",
      "step size index:32\n",
      "learning rate: 0.25\n",
      "loss: 0.004477  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005942 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:32\n",
      "learning rate: 0.25\n",
      "loss: 0.004446  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005921 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Previous best:0.8328\n",
      "step size index:33\n",
      "learning rate: 0.125\n",
      "loss: 0.004420  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005911 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Previous best:0.8328\n",
      "step size index:34\n",
      "learning rate: 0.0625\n",
      "loss: 0.004403  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005906 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Previous best:0.8328\n",
      "step size index:35\n",
      "learning rate: 0.03125\n",
      "loss: 0.004398  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005903 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:35\n",
      "learning rate: 0.03125\n",
      "loss: 0.004391  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005901 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:36\n",
      "learning rate: 0.015625\n",
      "loss: 0.004389  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005900 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Previous best:0.8328\n",
      "step size index:37\n",
      "learning rate: 0.0078125\n",
      "loss: 0.004386  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005899 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:37\n",
      "learning rate: 0.0078125\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005898 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Previous best:0.8328\n",
      "step size index:38\n",
      "learning rate: 0.00390625\n",
      "loss: 0.004385  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005898 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Previous best:0.8328\n",
      "step size index:39\n",
      "learning rate: 0.001953125\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005898 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Previous best:0.8328\n",
      "step size index:40\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:40\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:41\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:42\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:43\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:44\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:45\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:46\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:47\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:48\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:49\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:50\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:51\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:52\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:53\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:54\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:55\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:56\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:57\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:58\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:59\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005897 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:60\n",
      "learning rate: 1.0\n",
      "loss: 0.004384  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.005840 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Previous best:0.8308\n",
      "step size index:61\n",
      "learning rate: 0.5\n",
      "loss: 0.004435  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.005796 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Previous best:0.83\n",
      "step size index:62\n",
      "learning rate: 0.25\n",
      "loss: 0.004368  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.005774 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Previous best:0.8304\n",
      "step size index:62\n",
      "learning rate: 0.25\n",
      "loss: 0.004330  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.005753 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Previous best:0.8308\n",
      "step size index:62\n",
      "learning rate: 0.25\n",
      "loss: 0.004303  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005734 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Previous best:0.8316\n",
      "step size index:62\n",
      "learning rate: 0.25\n",
      "loss: 0.004281  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005716 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Previous best:0.8316\n",
      "step size index:63\n",
      "learning rate: 0.125\n",
      "loss: 0.004258  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005706 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Previous best:0.832\n",
      "step size index:63\n",
      "learning rate: 0.125\n",
      "loss: 0.004247  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005696 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Previous best:0.832\n",
      "step size index:64\n",
      "learning rate: 0.0625\n",
      "loss: 0.004238  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005692 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Previous best:0.832\n",
      "step size index:65\n",
      "learning rate: 0.03125\n",
      "loss: 0.004230  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005688 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Previous best:0.832\n",
      "step size index:66\n",
      "learning rate: 0.015625\n",
      "loss: 0.004225  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005688 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:66\n",
      "learning rate: 0.015625\n",
      "loss: 0.004224  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005686 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:67\n",
      "learning rate: 0.0078125\n",
      "loss: 0.004222  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005685 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Previous best:0.832\n",
      "step size index:68\n",
      "learning rate: 0.00390625\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:68\n",
      "learning rate: 0.00390625\n",
      "loss: 0.004222  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:69\n",
      "learning rate: 0.001953125\n",
      "loss: 0.004222  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:70\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:71\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:72\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:73\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:74\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:75\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:76\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:77\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:78\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:79\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:80\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:81\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:82\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:83\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:84\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:85\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:86\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:87\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:88\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:89\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005684 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:90\n",
      "learning rate: 1.0\n",
      "loss: 0.004221  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.005629 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Previous best:0.8304\n",
      "step size index:91\n",
      "learning rate: 0.5\n",
      "loss: 0.004295  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.005587 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:91\n",
      "learning rate: 0.5\n",
      "loss: 0.004231  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005550 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Previous best:0.8328\n",
      "step size index:91\n",
      "learning rate: 0.5\n",
      "loss: 0.004189  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005515 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:91\n",
      "learning rate: 0.5\n",
      "loss: 0.004161  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.005479 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Previous best:0.8328\n",
      "step size index:92\n",
      "learning rate: 0.25\n",
      "loss: 0.004137  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.005462 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Previous best:0.8336\n",
      "step size index:92\n",
      "learning rate: 0.25\n",
      "loss: 0.004108  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.005445 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Previous best:0.8348\n",
      "step size index:92\n",
      "learning rate: 0.25\n",
      "loss: 0.004086  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.005427 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Previous best:0.8352\n",
      "step size index:92\n",
      "learning rate: 0.25\n",
      "loss: 0.004068  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.005411 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Previous best:0.836\n",
      "step size index:92\n",
      "learning rate: 0.25\n",
      "loss: 0.004056  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.005394 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Previous best:0.8368\n",
      "step size index:92\n",
      "learning rate: 0.25\n",
      "loss: 0.004040  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.005378 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Previous best:0.836\n",
      "step size index:93\n",
      "learning rate: 0.125\n",
      "loss: 0.004030  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.005369 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Previous best:0.8376\n",
      "step size index:93\n",
      "learning rate: 0.125\n",
      "loss: 0.004018  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.005360 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Previous best:0.8376\n",
      "step size index:94\n",
      "learning rate: 0.0625\n",
      "loss: 0.004009  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.005355 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Previous best:0.8392\n",
      "step size index:94\n",
      "learning rate: 0.0625\n",
      "loss: 0.004006  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.005352 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Previous best:0.8372\n",
      "step size index:95\n",
      "learning rate: 0.03125\n",
      "loss: 0.003999  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.005351 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:95\n",
      "learning rate: 0.03125\n",
      "loss: 0.003996  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005348 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:95\n",
      "learning rate: 0.03125\n",
      "loss: 0.003996  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.005346 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:96\n",
      "learning rate: 0.015625\n",
      "loss: 0.003992  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.005346 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Previous best:0.8376\n",
      "step size index:97\n",
      "learning rate: 0.0078125\n",
      "loss: 0.003992  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.005345 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Previous best:0.8388\n",
      "step size index:97\n",
      "learning rate: 0.0078125\n",
      "loss: 0.003992  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.005345 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:98\n",
      "learning rate: 0.00390625\n",
      "loss: 0.003991  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.005344 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:99\n",
      "learning rate: 0.001953125\n",
      "loss: 0.003990  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.005344 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:100\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.003990  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005344 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "Previous best:0.8396\n",
      "step size index:100\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.003990  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005344 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:100\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.003990  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:101\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:102\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:103\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:104\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:105\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:106\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:107\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:108\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:109\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:110\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:111\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:112\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:113\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:114\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:115\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:116\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:117\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:118\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:119\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.005343 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "Previous best:0.84\n",
      "step size index:120\n",
      "learning rate: 1.0\n",
      "loss: 0.003989  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.005285 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "Previous best:0.8412\n",
      "step size index:120\n",
      "learning rate: 1.0\n",
      "loss: 0.004098  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.005223 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "Previous best:0.8432\n",
      "step size index:120\n",
      "learning rate: 1.0\n",
      "loss: 0.004089  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.005160 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "Previous best:0.8464\n",
      "step size index:120\n",
      "learning rate: 1.0\n",
      "loss: 0.004053  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.005100 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "Previous best:0.8476\n",
      "step size index:120\n",
      "learning rate: 1.0\n",
      "loss: 0.004028  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.005040 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "Previous best:0.8476\n",
      "step size index:121\n",
      "learning rate: 0.5\n",
      "loss: 0.003995  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.005009 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "Previous best:0.8492\n",
      "step size index:121\n",
      "learning rate: 0.5\n",
      "loss: 0.003911  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.004980 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "Previous best:0.848\n",
      "step size index:122\n",
      "learning rate: 0.25\n",
      "loss: 0.003872  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.004967 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "Previous best:0.8492\n",
      "step size index:122\n",
      "learning rate: 0.25\n",
      "loss: 0.003836  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.004954 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "Previous best:0.8492\n",
      "step size index:123\n",
      "learning rate: 0.125\n",
      "loss: 0.003813  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.004947 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "Previous best:0.8492\n",
      "step size index:124\n",
      "learning rate: 0.0625\n",
      "loss: 0.003799  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.004943 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "Previous best:0.8488\n",
      "step size index:125\n",
      "learning rate: 0.03125\n",
      "loss: 0.003792  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004942 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:125\n",
      "learning rate: 0.03125\n",
      "loss: 0.003788  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004941 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "Previous best:0.8504\n",
      "step size index:125\n",
      "learning rate: 0.03125\n",
      "loss: 0.003782  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.004939 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "Previous best:0.8484\n",
      "step size index:126\n",
      "learning rate: 0.015625\n",
      "loss: 0.003781  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.004939 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "Previous best:0.8492\n",
      "step size index:126\n",
      "learning rate: 0.015625\n",
      "loss: 0.003780  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "Previous best:0.8496\n",
      "step size index:126\n",
      "learning rate: 0.015625\n",
      "loss: 0.003780  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "Previous best:0.8492\n",
      "step size index:127\n",
      "learning rate: 0.0078125\n",
      "loss: 0.003778  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "Previous best:0.8492\n",
      "step size index:128\n",
      "learning rate: 0.00390625\n",
      "loss: 0.003778  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "Previous best:0.8496\n",
      "step size index:128\n",
      "learning rate: 0.00390625\n",
      "loss: 0.003778  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004936 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:128\n",
      "learning rate: 0.00390625\n",
      "loss: 0.003778  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004936 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:129\n",
      "learning rate: 0.001953125\n",
      "loss: 0.003778  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:130\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:131\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:132\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:133\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:134\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:135\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:136\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:137\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:138\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:139\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:140\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:141\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:142\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:143\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:144\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:145\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:146\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:147\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:148\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:149\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004937 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:150\n",
      "learning rate: 1.0\n",
      "loss: 0.003776  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.004876 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "Previous best:0.8484\n",
      "step size index:151\n",
      "learning rate: 0.5\n",
      "loss: 0.003893  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.004849 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "Previous best:0.8488\n",
      "step size index:151\n",
      "learning rate: 0.5\n",
      "loss: 0.003821  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004824 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:151\n",
      "learning rate: 0.5\n",
      "loss: 0.003787  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.004798 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "Previous best:0.8488\n",
      "step size index:152\n",
      "learning rate: 0.25\n",
      "loss: 0.003767  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004787 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "Previous best:0.8504\n",
      "step size index:152\n",
      "learning rate: 0.25\n",
      "loss: 0.003737  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004776 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "Previous best:0.8512\n",
      "step size index:152\n",
      "learning rate: 0.25\n",
      "loss: 0.003715  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004765 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:153\n",
      "learning rate: 0.125\n",
      "loss: 0.003701  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004758 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "Previous best:0.8512\n",
      "step size index:153\n",
      "learning rate: 0.125\n",
      "loss: 0.003690  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004753 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:154\n",
      "learning rate: 0.0625\n",
      "loss: 0.003679  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004750 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "Previous best:0.8504\n",
      "step size index:155\n",
      "learning rate: 0.03125\n",
      "loss: 0.003674  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004748 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "Previous best:0.85\n",
      "step size index:156\n",
      "learning rate: 0.015625\n",
      "loss: 0.003672  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.004747 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "Previous best:0.8504\n",
      "step size index:156\n",
      "learning rate: 0.015625\n",
      "loss: 0.003670  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004747 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:156\n",
      "learning rate: 0.015625\n",
      "loss: 0.003669  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004746 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "Previous best:0.8512\n",
      "step size index:156\n",
      "learning rate: 0.015625\n",
      "loss: 0.003668  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.004745 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "Previous best:0.8528\n",
      "step size index:156\n",
      "learning rate: 0.015625\n",
      "loss: 0.003669  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004744 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "Previous best:0.8512\n",
      "step size index:157\n",
      "learning rate: 0.0078125\n",
      "loss: 0.003668  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004744 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "Previous best:0.8512\n",
      "step size index:158\n",
      "learning rate: 0.00390625\n",
      "loss: 0.003669  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:159\n",
      "learning rate: 0.001953125\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "Previous best:0.8512\n",
      "step size index:159\n",
      "learning rate: 0.001953125\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004742 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "Previous best:0.8512\n",
      "step size index:160\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:161\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:162\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:163\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:164\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:165\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:166\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:167\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:168\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:169\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:170\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:171\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:172\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:173\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:174\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:175\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:176\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:177\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:178\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:179\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.1%, Avg loss: 0.004743 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "Previous best:0.8508\n",
      "step size index:180\n",
      "learning rate: 1.0\n",
      "loss: 0.003665  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.004682 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "Previous best:0.8472\n",
      "step size index:181\n",
      "learning rate: 0.5\n",
      "loss: 0.003799  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.004661 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "Previous best:0.8476\n",
      "step size index:181\n",
      "learning rate: 0.5\n",
      "loss: 0.003726  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.004639 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "Previous best:0.848\n",
      "step size index:181\n",
      "learning rate: 0.5\n",
      "loss: 0.003694  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.004616 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "Previous best:0.8468\n",
      "step size index:182\n",
      "learning rate: 0.25\n",
      "loss: 0.003676  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.8%, Avg loss: 0.004607 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "Previous best:0.8476\n",
      "step size index:182\n",
      "learning rate: 0.25\n",
      "loss: 0.003647  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.004596 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "Previous best:0.8452\n",
      "step size index:183\n",
      "learning rate: 0.125\n",
      "loss: 0.003626  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.004591 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "Previous best:0.846\n",
      "step size index:183\n",
      "learning rate: 0.125\n",
      "loss: 0.003617  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.004587 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "Previous best:0.8468\n",
      "step size index:183\n",
      "learning rate: 0.125\n",
      "loss: 0.003603  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.004583 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "Previous best:0.8452\n",
      "step size index:184\n",
      "learning rate: 0.0625\n",
      "loss: 0.003593  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.004581 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "Previous best:0.8432\n",
      "step size index:185\n",
      "learning rate: 0.03125\n",
      "loss: 0.003587  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004579 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "Previous best:0.844\n",
      "step size index:185\n",
      "learning rate: 0.03125\n",
      "loss: 0.003584  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004578 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:185\n",
      "learning rate: 0.03125\n",
      "loss: 0.003583  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.004578 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "Previous best:0.846\n",
      "step size index:185\n",
      "learning rate: 0.03125\n",
      "loss: 0.003578  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.004575 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "Previous best:0.8464\n",
      "step size index:185\n",
      "learning rate: 0.03125\n",
      "loss: 0.003579  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "Previous best:0.844\n",
      "step size index:186\n",
      "learning rate: 0.015625\n",
      "loss: 0.003578  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004574 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "Previous best:0.844\n",
      "step size index:187\n",
      "learning rate: 0.0078125\n",
      "loss: 0.003575  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:187\n",
      "learning rate: 0.0078125\n",
      "loss: 0.003572  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:188\n",
      "learning rate: 0.00390625\n",
      "loss: 0.003572  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:189\n",
      "learning rate: 0.001953125\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "Previous best:0.8448\n",
      "step size index:189\n",
      "learning rate: 0.001953125\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:190\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:191\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:192\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:193\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:194\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:195\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:196\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:197\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:198\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:199\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:200\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:201\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:202\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:203\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:204\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:205\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:206\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:207\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:208\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:209\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.004573 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "Previous best:0.8444\n",
      "step size index:210\n",
      "learning rate: 1.0\n",
      "loss: 0.003573  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.004510 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "Previous best:0.8392\n",
      "step size index:211\n",
      "learning rate: 0.5\n",
      "loss: 0.003717  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.004492 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "Previous best:0.8368\n",
      "step size index:212\n",
      "learning rate: 0.25\n",
      "loss: 0.003647  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.004485 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "Previous best:0.8372\n",
      "step size index:212\n",
      "learning rate: 0.25\n",
      "loss: 0.003609  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.004478 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Previous best:0.8372\n",
      "step size index:213\n",
      "learning rate: 0.125\n",
      "loss: 0.003582  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004474 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:213\n",
      "learning rate: 0.125\n",
      "loss: 0.003566  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004469 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "Previous best:0.8376\n",
      "step size index:214\n",
      "learning rate: 0.0625\n",
      "loss: 0.003552  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004467 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "Previous best:0.838\n",
      "step size index:214\n",
      "learning rate: 0.0625\n",
      "loss: 0.003548  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.004466 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "Previous best:0.8368\n",
      "step size index:215\n",
      "learning rate: 0.03125\n",
      "loss: 0.003539  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.004464 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "Previous best:0.8368\n",
      "step size index:216\n",
      "learning rate: 0.015625\n",
      "loss: 0.003537  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004464 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:216\n",
      "learning rate: 0.015625\n",
      "loss: 0.003536  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004464 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:217\n",
      "learning rate: 0.0078125\n",
      "loss: 0.003533  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "Previous best:0.8388\n",
      "step size index:217\n",
      "learning rate: 0.0078125\n",
      "loss: 0.003530  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "Previous best:0.838\n",
      "step size index:218\n",
      "learning rate: 0.00390625\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "Previous best:0.838\n",
      "step size index:219\n",
      "learning rate: 0.001953125\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "Previous best:0.838\n",
      "step size index:220\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "Previous best:0.838\n",
      "step size index:221\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:221\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:222\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:223\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:224\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:225\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:226\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:227\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:228\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:229\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:230\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:231\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:232\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:233\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:234\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:235\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:236\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:237\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:238\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:239\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.004465 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "Previous best:0.8384\n",
      "step size index:240\n",
      "learning rate: 1.0\n",
      "loss: 0.003528  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.004401 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "Previous best:0.8348\n",
      "step size index:241\n",
      "learning rate: 0.5\n",
      "loss: 0.003668  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.004387 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "Previous best:0.8352\n",
      "step size index:241\n",
      "learning rate: 0.5\n",
      "loss: 0.003598  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.004370 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "Previous best:0.8344\n",
      "step size index:242\n",
      "learning rate: 0.25\n",
      "loss: 0.003566  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.004364 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "Previous best:0.832\n",
      "step size index:243\n",
      "learning rate: 0.125\n",
      "loss: 0.003532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.004362 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "Previous best:0.8336\n",
      "step size index:243\n",
      "learning rate: 0.125\n",
      "loss: 0.003514  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004358 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:244\n",
      "learning rate: 0.0625\n",
      "loss: 0.003503  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.004357 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:245\n",
      "learning rate: 0.03125\n",
      "loss: 0.003493  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004357 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:245\n",
      "learning rate: 0.03125\n",
      "loss: 0.003488  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.004356 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:246\n",
      "learning rate: 0.015625\n",
      "loss: 0.003486  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004355 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "Previous best:0.8328\n",
      "step size index:246\n",
      "learning rate: 0.015625\n",
      "loss: 0.003485  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004355 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:246\n",
      "learning rate: 0.015625\n",
      "loss: 0.003483  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.004355 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:247\n",
      "learning rate: 0.0078125\n",
      "loss: 0.003480  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.004355 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "Previous best:0.832\n",
      "step size index:248\n",
      "learning rate: 0.00390625\n",
      "loss: 0.003480  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.004355 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "Previous best:0.8324\n",
      "step size index:248\n",
      "learning rate: 0.00390625\n",
      "loss: 0.003480  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004355 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "Previous best:0.8328\n",
      "step size index:248\n",
      "learning rate: 0.00390625\n",
      "loss: 0.003480  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004353 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:248\n",
      "learning rate: 0.00390625\n",
      "loss: 0.003481  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:249\n",
      "learning rate: 0.001953125\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:250\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:251\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:252\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:253\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:254\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:255\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:256\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:257\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:258\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:259\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:260\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:261\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:262\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:263\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:264\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:265\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:266\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:267\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:268\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:269\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004354 \n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "Previous best:0.8332\n",
      "step size index:270\n",
      "learning rate: 1.0\n",
      "loss: 0.003479  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004292 \n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "Previous best:0.83\n",
      "step size index:271\n",
      "learning rate: 0.5\n",
      "loss: 0.003616  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.004281 \n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "Previous best:0.8308\n",
      "step size index:271\n",
      "learning rate: 0.5\n",
      "loss: 0.003545  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.2%, Avg loss: 0.004264 \n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "Previous best:0.8316\n",
      "step size index:271\n",
      "learning rate: 0.5\n",
      "loss: 0.003513  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.004248 \n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "Previous best:0.828\n",
      "step size index:272\n",
      "learning rate: 0.25\n",
      "loss: 0.003498  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004243 \n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:272\n",
      "learning rate: 0.25\n",
      "loss: 0.003465  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004236 \n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:273\n",
      "learning rate: 0.125\n",
      "loss: 0.003451  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004234 \n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "Previous best:0.8304\n",
      "step size index:273\n",
      "learning rate: 0.125\n",
      "loss: 0.003435  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004231 \n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "Previous best:0.8304\n",
      "step size index:274\n",
      "learning rate: 0.0625\n",
      "loss: 0.003424  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004230 \n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:275\n",
      "learning rate: 0.03125\n",
      "loss: 0.003420  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.004228 \n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "Previous best:0.8272\n",
      "step size index:276\n",
      "learning rate: 0.015625\n",
      "loss: 0.003416  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004228 \n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:276\n",
      "learning rate: 0.015625\n",
      "loss: 0.003413  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.004228 \n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "Previous best:0.8288\n",
      "step size index:277\n",
      "learning rate: 0.0078125\n",
      "loss: 0.003410  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "Previous best:0.83\n",
      "step size index:277\n",
      "learning rate: 0.0078125\n",
      "loss: 0.003410  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:278\n",
      "learning rate: 0.00390625\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "Previous best:0.8292\n",
      "step size index:279\n",
      "learning rate: 0.001953125\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.004228 \n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "Previous best:0.8292\n",
      "step size index:280\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.004228 \n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "Previous best:0.8292\n",
      "step size index:281\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:281\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004228 \n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:282\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:283\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:284\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:285\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:286\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:287\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:288\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:289\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:290\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:291\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:292\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:293\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:294\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:295\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:296\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:297\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:298\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:299\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004227 \n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:300\n",
      "learning rate: 1.0\n",
      "loss: 0.003407  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.004164 \n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "Previous best:0.828\n",
      "step size index:301\n",
      "learning rate: 0.5\n",
      "loss: 0.003555  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.004155 \n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "Previous best:0.8292\n",
      "step size index:301\n",
      "learning rate: 0.5\n",
      "loss: 0.003482  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004142 \n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:301\n",
      "learning rate: 0.5\n",
      "loss: 0.003455  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.004127 \n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "Previous best:0.828\n",
      "step size index:302\n",
      "learning rate: 0.25\n",
      "loss: 0.003435  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004123 \n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "Previous best:0.8256\n",
      "step size index:303\n",
      "learning rate: 0.125\n",
      "loss: 0.003406  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004123 \n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "Previous best:0.8296\n",
      "step size index:303\n",
      "learning rate: 0.125\n",
      "loss: 0.003388  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.004118 \n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "Previous best:0.8292\n",
      "step size index:304\n",
      "learning rate: 0.0625\n",
      "loss: 0.003381  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.0%, Avg loss: 0.004118 \n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "Previous best:0.83\n",
      "step size index:304\n",
      "learning rate: 0.0625\n",
      "loss: 0.003371  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.004118 \n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "Previous best:0.8328\n",
      "step size index:304\n",
      "learning rate: 0.0625\n",
      "loss: 0.003362  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.004117 \n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "Previous best:0.8272\n",
      "step size index:305\n",
      "learning rate: 0.03125\n",
      "loss: 0.003358  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.004116 \n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "Previous best:0.8272\n",
      "step size index:306\n",
      "learning rate: 0.015625\n",
      "loss: 0.003356  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "Previous best:0.8272\n",
      "step size index:307\n",
      "learning rate: 0.0078125\n",
      "loss: 0.003354  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.004116 \n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "Previous best:0.8272\n",
      "step size index:308\n",
      "learning rate: 0.00390625\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.004116 \n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "Previous best:0.828\n",
      "step size index:308\n",
      "learning rate: 0.00390625\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.004116 \n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "Previous best:0.828\n",
      "step size index:309\n",
      "learning rate: 0.001953125\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.004116 \n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "Previous best:0.828\n",
      "step size index:310\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "Previous best:0.8268\n",
      "step size index:311\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:312\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:313\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:314\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:315\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:316\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:317\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:318\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:319\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:320\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:321\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:322\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:323\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:324\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:325\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:326\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:327\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:328\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:329\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004115 \n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:330\n",
      "learning rate: 1.0\n",
      "loss: 0.003353  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.004053 \n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "Previous best:0.8268\n",
      "step size index:330\n",
      "learning rate: 1.0\n",
      "loss: 0.003500  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.004018 \n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "Previous best:0.8248\n",
      "step size index:331\n",
      "learning rate: 0.5\n",
      "loss: 0.003501  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.004015 \n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "Previous best:0.8228\n",
      "step size index:332\n",
      "learning rate: 0.25\n",
      "loss: 0.003415  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.004013 \n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "Previous best:0.8236\n",
      "step size index:332\n",
      "learning rate: 0.25\n",
      "loss: 0.003373  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.7%, Avg loss: 0.004010 \n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "Previous best:0.8268\n",
      "step size index:332\n",
      "learning rate: 0.25\n",
      "loss: 0.003347  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004007 \n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "Previous best:0.8264\n",
      "step size index:333\n",
      "learning rate: 0.125\n",
      "loss: 0.003330  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 0.004004 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Previous best:0.824\n",
      "step size index:334\n",
      "learning rate: 0.0625\n",
      "loss: 0.003320  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.004004 \n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(100)\n",
    "\n",
    "best_model = NeuralNetwork().to(device)\n",
    "\n",
    "#best_model.apply(weights_init)\n",
    "model_1 =test_train(0,end_epoch,0,0,0,best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d0ef18-7329-48fd-95bf-627a347e7a52",
   "metadata": {},
   "source": [
    "Very good! High accuracy but lower avg loss then the previous approach. Let's save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e53c0d19-faba-4edd-a69b-a98dc93a25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_1.state_dict(), \"model_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "920b96ca-2027-40e3-8459-1445de514a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = NeuralNetwork().to(device)\n",
    "model_1.load_state_dict(torch.load(\"model_1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29dd44be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_1.linear_relu_stack[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a29cae-067d-43fb-bded-9358e2570a26",
   "metadata": {},
   "source": [
    "Now here's the thing: the performance of a neural network can be is highly dependent on the seed with which its weights were initialized.\n",
    "\n",
    "You can think of the parameter space as a valley with the random weight initializer being Herculus throwing a discus in a hilly plain. \n",
    "If the discuss lands in a spot that's proper for SGD to do its job, or else it might just be stuck at a local maxima for a very very long time. \n",
    "\n",
    "To illustrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d46f3f8c-139a-4ecd-8f67-f0a0f7cf0850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "Previous best:0\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:1\n",
      "learning rate: 0.5\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:2\n",
      "learning rate: 0.25\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:3\n",
      "learning rate: 0.125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:4\n",
      "learning rate: 0.0625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:5\n",
      "learning rate: 0.03125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:6\n",
      "learning rate: 0.015625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:7\n",
      "learning rate: 0.0078125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:8\n",
      "learning rate: 0.00390625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:9\n",
      "learning rate: 0.001953125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:10\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:11\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:12\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:13\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:14\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:15\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:16\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:17\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:18\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:19\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:20\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:21\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:22\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:23\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:24\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:25\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:26\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:27\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:28\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:29\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:31\n",
      "learning rate: 0.5\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:32\n",
      "learning rate: 0.25\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:33\n",
      "learning rate: 0.125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:34\n",
      "learning rate: 0.0625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:35\n",
      "learning rate: 0.03125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:36\n",
      "learning rate: 0.015625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:37\n",
      "learning rate: 0.0078125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:38\n",
      "learning rate: 0.00390625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:39\n",
      "learning rate: 0.001953125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:40\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:41\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:42\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:43\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:44\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:45\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:46\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:47\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:48\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:49\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:50\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:51\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:52\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:53\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:54\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:55\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:56\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:57\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:58\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:59\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:60\n",
      "learning rate: 1.0\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:61\n",
      "learning rate: 0.5\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:62\n",
      "learning rate: 0.25\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:63\n",
      "learning rate: 0.125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:64\n",
      "learning rate: 0.0625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:65\n",
      "learning rate: 0.03125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:66\n",
      "learning rate: 0.015625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:67\n",
      "learning rate: 0.0078125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:68\n",
      "learning rate: 0.00390625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:69\n",
      "learning rate: 0.001953125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:70\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:71\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:72\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:73\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:74\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:75\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:76\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:77\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:78\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:79\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:80\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:81\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:82\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:83\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:84\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:85\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:86\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:87\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:88\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:89\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:90\n",
      "learning rate: 1.0\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:91\n",
      "learning rate: 0.5\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:92\n",
      "learning rate: 0.25\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:93\n",
      "learning rate: 0.125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:94\n",
      "learning rate: 0.0625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:95\n",
      "learning rate: 0.03125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:96\n",
      "learning rate: 0.015625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:97\n",
      "learning rate: 0.0078125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:98\n",
      "learning rate: 0.00390625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:99\n",
      "learning rate: 0.001953125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:100\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:101\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:102\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:103\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:104\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:105\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:106\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:107\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:108\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:109\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:110\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:111\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:112\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:113\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:114\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:115\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:116\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:117\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:118\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:119\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:120\n",
      "learning rate: 1.0\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:121\n",
      "learning rate: 0.5\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:122\n",
      "learning rate: 0.25\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:123\n",
      "learning rate: 0.125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:124\n",
      "learning rate: 0.0625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:125\n",
      "learning rate: 0.03125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:126\n",
      "learning rate: 0.015625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:127\n",
      "learning rate: 0.0078125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:128\n",
      "learning rate: 0.00390625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:129\n",
      "learning rate: 0.001953125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:130\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:131\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:132\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:133\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:134\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:135\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:136\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:137\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:138\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:139\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:140\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:141\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:142\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:143\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:144\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:145\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:146\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:147\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:148\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:149\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:150\n",
      "learning rate: 1.0\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:151\n",
      "learning rate: 0.5\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:152\n",
      "learning rate: 0.25\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:153\n",
      "learning rate: 0.125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:154\n",
      "learning rate: 0.0625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:155\n",
      "learning rate: 0.03125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:156\n",
      "learning rate: 0.015625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:157\n",
      "learning rate: 0.0078125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:158\n",
      "learning rate: 0.00390625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:159\n",
      "learning rate: 0.001953125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:160\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:161\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:162\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:163\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:164\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:165\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:166\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:167\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:168\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:169\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:170\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:171\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:172\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:173\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:174\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:175\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:176\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:177\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:178\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:179\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:180\n",
      "learning rate: 1.0\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:181\n",
      "learning rate: 0.5\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:182\n",
      "learning rate: 0.25\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:183\n",
      "learning rate: 0.125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:184\n",
      "learning rate: 0.0625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:185\n",
      "learning rate: 0.03125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:186\n",
      "learning rate: 0.015625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:187\n",
      "learning rate: 0.0078125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:188\n",
      "learning rate: 0.00390625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:189\n",
      "learning rate: 0.001953125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:190\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:191\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:192\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:193\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:194\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:195\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:196\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:197\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:198\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:199\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:200\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:201\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:202\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:203\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:204\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:205\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:206\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:207\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:208\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:209\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:210\n",
      "learning rate: 1.0\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:211\n",
      "learning rate: 0.5\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:212\n",
      "learning rate: 0.25\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:213\n",
      "learning rate: 0.125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:214\n",
      "learning rate: 0.0625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:215\n",
      "learning rate: 0.03125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:216\n",
      "learning rate: 0.015625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:217\n",
      "learning rate: 0.0078125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:218\n",
      "learning rate: 0.00390625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:219\n",
      "learning rate: 0.001953125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:220\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:221\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:222\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:223\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:224\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:225\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:226\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:227\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:228\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:229\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:230\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:231\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:232\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:233\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:234\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:235\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:236\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:237\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:238\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:239\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:240\n",
      "learning rate: 1.0\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:241\n",
      "learning rate: 0.5\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:242\n",
      "learning rate: 0.25\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:243\n",
      "learning rate: 0.125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:244\n",
      "learning rate: 0.0625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:245\n",
      "learning rate: 0.03125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:246\n",
      "learning rate: 0.015625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:247\n",
      "learning rate: 0.0078125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:248\n",
      "learning rate: 0.00390625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:249\n",
      "learning rate: 0.001953125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:250\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:251\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:252\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:253\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:254\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:255\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:256\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:257\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:258\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:259\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:260\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:261\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:262\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:263\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:264\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:265\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:266\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:267\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:268\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:269\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:270\n",
      "learning rate: 1.0\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:271\n",
      "learning rate: 0.5\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:272\n",
      "learning rate: 0.25\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:273\n",
      "learning rate: 0.125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:274\n",
      "learning rate: 0.0625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:275\n",
      "learning rate: 0.03125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:276\n",
      "learning rate: 0.015625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:277\n",
      "learning rate: 0.0078125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:278\n",
      "learning rate: 0.00390625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:279\n",
      "learning rate: 0.001953125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:280\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:281\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:282\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:283\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:284\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:285\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:286\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:287\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:288\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:289\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:290\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:291\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:292\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:293\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:294\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:295\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:296\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:297\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:298\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:299\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:300\n",
      "learning rate: 1.0\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:301\n",
      "learning rate: 0.5\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:302\n",
      "learning rate: 0.25\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:303\n",
      "learning rate: 0.125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:304\n",
      "learning rate: 0.0625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:305\n",
      "learning rate: 0.03125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:306\n",
      "learning rate: 0.015625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:307\n",
      "learning rate: 0.0078125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:308\n",
      "learning rate: 0.00390625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:309\n",
      "learning rate: 0.001953125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:310\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:311\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:312\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:313\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:314\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:315\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:316\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:317\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:318\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:319\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:320\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:321\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:322\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:323\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:324\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:325\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:326\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:327\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:328\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:329\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:330\n",
      "learning rate: 1.0\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:331\n",
      "learning rate: 0.5\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:332\n",
      "learning rate: 0.25\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:333\n",
      "learning rate: 0.125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:334\n",
      "learning rate: 0.0625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:335\n",
      "learning rate: 0.03125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:336\n",
      "learning rate: 0.015625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:337\n",
      "learning rate: 0.0078125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:338\n",
      "learning rate: 0.00390625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:339\n",
      "learning rate: 0.001953125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:340\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:341\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:342\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:343\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:344\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:345\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:346\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:347\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:348\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:349\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:350\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:351\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:352\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:353\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:354\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:355\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:356\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:357\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:358\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:359\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:360\n",
      "learning rate: 1.0\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:361\n",
      "learning rate: 0.5\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:362\n",
      "learning rate: 0.25\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:363\n",
      "learning rate: 0.125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:364\n",
      "learning rate: 0.0625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:365\n",
      "learning rate: 0.03125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:366\n",
      "learning rate: 0.015625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:367\n",
      "learning rate: 0.0078125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:368\n",
      "learning rate: 0.00390625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:369\n",
      "learning rate: 0.001953125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:370\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:371\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:372\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:373\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:374\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:375\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:376\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:377\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:378\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:379\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:380\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:381\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:382\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:383\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:384\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:385\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:386\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:387\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:388\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:389\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:390\n",
      "learning rate: 1.0\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:391\n",
      "learning rate: 0.5\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:392\n",
      "learning rate: 0.25\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:393\n",
      "learning rate: 0.125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:394\n",
      "learning rate: 0.0625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:395\n",
      "learning rate: 0.03125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:396\n",
      "learning rate: 0.015625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:397\n",
      "learning rate: 0.0078125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:398\n",
      "learning rate: 0.00390625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:399\n",
      "learning rate: 0.001953125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:400\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:401\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:402\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:403\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:404\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:405\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:406\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:407\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:408\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:409\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:410\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:411\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:412\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:413\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:414\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:415\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:416\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:417\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:418\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:419\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:420\n",
      "learning rate: 1.0\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:421\n",
      "learning rate: 0.5\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:422\n",
      "learning rate: 0.25\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:423\n",
      "learning rate: 0.125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:424\n",
      "learning rate: 0.0625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:425\n",
      "learning rate: 0.03125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:426\n",
      "learning rate: 0.015625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:427\n",
      "learning rate: 0.0078125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:428\n",
      "learning rate: 0.00390625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:429\n",
      "learning rate: 0.001953125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:430\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:431\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:432\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:433\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:434\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:435\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:436\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:437\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:438\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:439\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:440\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:441\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:442\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:443\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:444\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:445\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:446\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:447\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:448\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:449\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:450\n",
      "learning rate: 1.0\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:451\n",
      "learning rate: 0.5\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:452\n",
      "learning rate: 0.25\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:453\n",
      "learning rate: 0.125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:454\n",
      "learning rate: 0.0625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:455\n",
      "learning rate: 0.03125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:456\n",
      "learning rate: 0.015625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:457\n",
      "learning rate: 0.0078125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:458\n",
      "learning rate: 0.00390625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:459\n",
      "learning rate: 0.001953125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:460\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:461\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:462\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:463\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:464\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:465\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:466\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:467\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:468\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:469\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:470\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:471\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:472\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:473\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:474\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:475\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:476\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:477\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:478\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:479\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:480\n",
      "learning rate: 1.0\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:481\n",
      "learning rate: 0.5\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:482\n",
      "learning rate: 0.25\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:483\n",
      "learning rate: 0.125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:484\n",
      "learning rate: 0.0625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:485\n",
      "learning rate: 0.03125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:486\n",
      "learning rate: 0.015625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:487\n",
      "learning rate: 0.0078125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:488\n",
      "learning rate: 0.00390625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:489\n",
      "learning rate: 0.001953125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:490\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:491\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:492\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:493\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:494\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:495\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:496\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:497\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:498\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Previous best:0.2656\n",
      "step size index:499\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.089532  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 26.6%, Avg loss: 1.088265 \n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(5)\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "best_model = NeuralNetwork().to(device)\n",
    "\n",
    "#best_model.apply(weights_init)\n",
    "model_2 =test_train(0,end_epoch,0,0,0,best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d80d49b-d204-49d2-9bb0-fb2ab1e7e2f2",
   "metadata": {},
   "source": [
    "No where near as good. \n",
    "\n",
    "Right so this experiment didn't lead anywhere.\n",
    "Let's try another random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b570a782-3108-4da9-809b-4b61f113d8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "Previous best:0\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:1\n",
      "learning rate: 0.5\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:2\n",
      "learning rate: 0.25\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:3\n",
      "learning rate: 0.125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:4\n",
      "learning rate: 0.0625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:5\n",
      "learning rate: 0.03125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:6\n",
      "learning rate: 0.015625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:7\n",
      "learning rate: 0.0078125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:8\n",
      "learning rate: 0.00390625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:9\n",
      "learning rate: 0.001953125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:10\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:11\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:12\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:13\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:14\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:15\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:16\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:17\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:18\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:19\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:20\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:21\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:22\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:23\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:24\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:25\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:26\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:27\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:28\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:29\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:31\n",
      "learning rate: 0.5\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:32\n",
      "learning rate: 0.25\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:33\n",
      "learning rate: 0.125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:34\n",
      "learning rate: 0.0625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:35\n",
      "learning rate: 0.03125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:36\n",
      "learning rate: 0.015625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:37\n",
      "learning rate: 0.0078125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:38\n",
      "learning rate: 0.00390625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:39\n",
      "learning rate: 0.001953125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:40\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:41\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:42\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:43\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:44\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:45\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:46\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:47\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:48\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:49\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:50\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:51\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:52\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:53\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:54\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:55\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:56\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:57\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:58\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:59\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:60\n",
      "learning rate: 1.0\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:61\n",
      "learning rate: 0.5\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:62\n",
      "learning rate: 0.25\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:63\n",
      "learning rate: 0.125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:64\n",
      "learning rate: 0.0625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:65\n",
      "learning rate: 0.03125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:66\n",
      "learning rate: 0.015625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:67\n",
      "learning rate: 0.0078125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:68\n",
      "learning rate: 0.00390625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:69\n",
      "learning rate: 0.001953125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:70\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:71\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:72\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:73\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:74\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:75\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:76\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:77\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:78\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:79\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:80\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:81\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:82\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:83\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:84\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:85\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:86\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:87\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:88\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:89\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:90\n",
      "learning rate: 1.0\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:91\n",
      "learning rate: 0.5\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:92\n",
      "learning rate: 0.25\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:93\n",
      "learning rate: 0.125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:94\n",
      "learning rate: 0.0625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:95\n",
      "learning rate: 0.03125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:96\n",
      "learning rate: 0.015625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:97\n",
      "learning rate: 0.0078125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:98\n",
      "learning rate: 0.00390625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:99\n",
      "learning rate: 0.001953125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:100\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:101\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:102\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:103\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:104\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:105\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:106\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:107\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:108\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:109\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:110\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:111\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:112\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:113\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:114\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:115\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:116\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:117\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:118\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:119\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:120\n",
      "learning rate: 1.0\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:121\n",
      "learning rate: 0.5\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:122\n",
      "learning rate: 0.25\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:123\n",
      "learning rate: 0.125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:124\n",
      "learning rate: 0.0625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:125\n",
      "learning rate: 0.03125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:126\n",
      "learning rate: 0.015625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:127\n",
      "learning rate: 0.0078125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:128\n",
      "learning rate: 0.00390625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:129\n",
      "learning rate: 0.001953125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:130\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:131\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:132\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:133\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:134\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:135\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:136\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:137\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:138\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:139\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:140\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:141\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:142\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:143\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:144\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:145\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:146\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:147\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:148\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:149\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:150\n",
      "learning rate: 1.0\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:151\n",
      "learning rate: 0.5\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:152\n",
      "learning rate: 0.25\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:153\n",
      "learning rate: 0.125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:154\n",
      "learning rate: 0.0625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:155\n",
      "learning rate: 0.03125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:156\n",
      "learning rate: 0.015625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:157\n",
      "learning rate: 0.0078125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:158\n",
      "learning rate: 0.00390625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:159\n",
      "learning rate: 0.001953125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:160\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:161\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:162\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:163\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:164\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:165\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:166\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:167\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:168\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:169\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:170\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:171\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:172\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:173\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:174\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:175\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:176\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:177\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:178\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:179\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:180\n",
      "learning rate: 1.0\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:181\n",
      "learning rate: 0.5\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:182\n",
      "learning rate: 0.25\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:183\n",
      "learning rate: 0.125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:184\n",
      "learning rate: 0.0625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:185\n",
      "learning rate: 0.03125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:186\n",
      "learning rate: 0.015625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:187\n",
      "learning rate: 0.0078125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:188\n",
      "learning rate: 0.00390625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:189\n",
      "learning rate: 0.001953125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:190\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:191\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:192\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:193\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:194\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:195\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:196\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:197\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:198\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:199\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:200\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:201\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:202\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:203\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:204\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:205\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:206\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:207\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:208\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:209\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:210\n",
      "learning rate: 1.0\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:211\n",
      "learning rate: 0.5\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:212\n",
      "learning rate: 0.25\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:213\n",
      "learning rate: 0.125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:214\n",
      "learning rate: 0.0625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:215\n",
      "learning rate: 0.03125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:216\n",
      "learning rate: 0.015625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:217\n",
      "learning rate: 0.0078125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:218\n",
      "learning rate: 0.00390625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:219\n",
      "learning rate: 0.001953125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:220\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:221\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:222\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:223\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:224\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:225\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:226\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:227\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:228\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:229\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:230\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:231\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:232\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:233\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:234\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:235\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:236\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:237\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:238\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:239\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:240\n",
      "learning rate: 1.0\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:241\n",
      "learning rate: 0.5\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:242\n",
      "learning rate: 0.25\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:243\n",
      "learning rate: 0.125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:244\n",
      "learning rate: 0.0625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:245\n",
      "learning rate: 0.03125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:246\n",
      "learning rate: 0.015625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:247\n",
      "learning rate: 0.0078125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:248\n",
      "learning rate: 0.00390625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:249\n",
      "learning rate: 0.001953125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:250\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:251\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:252\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:253\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:254\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:255\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:256\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:257\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:258\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:259\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:260\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:261\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:262\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:263\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:264\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:265\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:266\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:267\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:268\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:269\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:270\n",
      "learning rate: 1.0\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:271\n",
      "learning rate: 0.5\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:272\n",
      "learning rate: 0.25\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:273\n",
      "learning rate: 0.125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:274\n",
      "learning rate: 0.0625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:275\n",
      "learning rate: 0.03125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:276\n",
      "learning rate: 0.015625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:277\n",
      "learning rate: 0.0078125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:278\n",
      "learning rate: 0.00390625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:279\n",
      "learning rate: 0.001953125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:280\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:281\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:282\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:283\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:284\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:285\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:286\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:287\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:288\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:289\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:290\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:291\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:292\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:293\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:294\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:295\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:296\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:297\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:298\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:299\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:300\n",
      "learning rate: 1.0\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:301\n",
      "learning rate: 0.5\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:302\n",
      "learning rate: 0.25\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:303\n",
      "learning rate: 0.125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:304\n",
      "learning rate: 0.0625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:305\n",
      "learning rate: 0.03125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:306\n",
      "learning rate: 0.015625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:307\n",
      "learning rate: 0.0078125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:308\n",
      "learning rate: 0.00390625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:309\n",
      "learning rate: 0.001953125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:310\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:311\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:312\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:313\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:314\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:315\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:316\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:317\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:318\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:319\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:320\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:321\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:322\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:323\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:324\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:325\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:326\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:327\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:328\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:329\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:330\n",
      "learning rate: 1.0\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:331\n",
      "learning rate: 0.5\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:332\n",
      "learning rate: 0.25\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:333\n",
      "learning rate: 0.125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:334\n",
      "learning rate: 0.0625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:335\n",
      "learning rate: 0.03125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:336\n",
      "learning rate: 0.015625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:337\n",
      "learning rate: 0.0078125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:338\n",
      "learning rate: 0.00390625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:339\n",
      "learning rate: 0.001953125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:340\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:341\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:342\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:343\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:344\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:345\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:346\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:347\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:348\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:349\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:350\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:351\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:352\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:353\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:354\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:355\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:356\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:357\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:358\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:359\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:360\n",
      "learning rate: 1.0\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:361\n",
      "learning rate: 0.5\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:362\n",
      "learning rate: 0.25\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:363\n",
      "learning rate: 0.125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:364\n",
      "learning rate: 0.0625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:365\n",
      "learning rate: 0.03125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:366\n",
      "learning rate: 0.015625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:367\n",
      "learning rate: 0.0078125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:368\n",
      "learning rate: 0.00390625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:369\n",
      "learning rate: 0.001953125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:370\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:371\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:372\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:373\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:374\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:375\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:376\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:377\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:378\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:379\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:380\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:381\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:382\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:383\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:384\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:385\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:386\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:387\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:388\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:389\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:390\n",
      "learning rate: 1.0\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:391\n",
      "learning rate: 0.5\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:392\n",
      "learning rate: 0.25\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:393\n",
      "learning rate: 0.125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:394\n",
      "learning rate: 0.0625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:395\n",
      "learning rate: 0.03125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:396\n",
      "learning rate: 0.015625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:397\n",
      "learning rate: 0.0078125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:398\n",
      "learning rate: 0.00390625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:399\n",
      "learning rate: 0.001953125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:400\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:401\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:402\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:403\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:404\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:405\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:406\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:407\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:408\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:409\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:410\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:411\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:412\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:413\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:414\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:415\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:416\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:417\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:418\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:419\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:420\n",
      "learning rate: 1.0\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:421\n",
      "learning rate: 0.5\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:422\n",
      "learning rate: 0.25\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:423\n",
      "learning rate: 0.125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:424\n",
      "learning rate: 0.0625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:425\n",
      "learning rate: 0.03125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:426\n",
      "learning rate: 0.015625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:427\n",
      "learning rate: 0.0078125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:428\n",
      "learning rate: 0.00390625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:429\n",
      "learning rate: 0.001953125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:430\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:431\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:432\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:433\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:434\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:435\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:436\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:437\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:438\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:439\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:440\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:441\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:442\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:443\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:444\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:445\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:446\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:447\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:448\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:449\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:450\n",
      "learning rate: 1.0\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:451\n",
      "learning rate: 0.5\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:452\n",
      "learning rate: 0.25\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:453\n",
      "learning rate: 0.125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:454\n",
      "learning rate: 0.0625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:455\n",
      "learning rate: 0.03125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:456\n",
      "learning rate: 0.015625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:457\n",
      "learning rate: 0.0078125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:458\n",
      "learning rate: 0.00390625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:459\n",
      "learning rate: 0.001953125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:460\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:461\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:462\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:463\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:464\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:465\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:466\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:467\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:468\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:469\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:470\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:471\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:472\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:473\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:474\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:475\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:476\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:477\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:478\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:479\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:480\n",
      "learning rate: 1.0\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:481\n",
      "learning rate: 0.5\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:482\n",
      "learning rate: 0.25\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:483\n",
      "learning rate: 0.125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:484\n",
      "learning rate: 0.0625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:485\n",
      "learning rate: 0.03125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:486\n",
      "learning rate: 0.015625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:487\n",
      "learning rate: 0.0078125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:488\n",
      "learning rate: 0.00390625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:489\n",
      "learning rate: 0.001953125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:490\n",
      "learning rate: 0.0009765625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:491\n",
      "learning rate: 0.00048828125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:492\n",
      "learning rate: 0.000244140625\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:493\n",
      "learning rate: 0.0001220703125\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:494\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:495\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:496\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:497\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:498\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Previous best:0.7812\n",
      "step size index:499\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 0.657311  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 0.657997 \n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(200)\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "best_model = NeuralNetwork().to(device)\n",
    "\n",
    "#best_model.apply(weights_init)\n",
    "model_3 =test_train(0,end_epoch,0,0,0,best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca03ccd-c75a-4d28-94ad-31f26c8e1e60",
   "metadata": {},
   "source": [
    "Better, but can we get something closer to what we want? \n",
    "Let's try something else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bc884d5c-3431-4e58-9cdb-5f2c3b37b31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "Previous best:0\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:1\n",
      "learning rate: 0.5\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:2\n",
      "learning rate: 0.25\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:3\n",
      "learning rate: 0.125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:4\n",
      "learning rate: 0.0625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:5\n",
      "learning rate: 0.03125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:6\n",
      "learning rate: 0.015625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:7\n",
      "learning rate: 0.0078125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:8\n",
      "learning rate: 0.00390625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:9\n",
      "learning rate: 0.001953125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:10\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:11\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:12\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:13\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:14\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:15\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:16\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:17\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:18\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:19\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:20\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:21\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:22\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:23\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:24\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:25\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:26\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:27\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:28\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:29\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:31\n",
      "learning rate: 0.5\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:32\n",
      "learning rate: 0.25\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:33\n",
      "learning rate: 0.125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:34\n",
      "learning rate: 0.0625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:35\n",
      "learning rate: 0.03125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:36\n",
      "learning rate: 0.015625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:37\n",
      "learning rate: 0.0078125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:38\n",
      "learning rate: 0.00390625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:39\n",
      "learning rate: 0.001953125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:40\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:41\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:42\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:43\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:44\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:45\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:46\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:47\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:48\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:49\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:50\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:51\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:52\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:53\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:54\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:55\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:56\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:57\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:58\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:59\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:60\n",
      "learning rate: 1.0\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:61\n",
      "learning rate: 0.5\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:62\n",
      "learning rate: 0.25\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:63\n",
      "learning rate: 0.125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:64\n",
      "learning rate: 0.0625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:65\n",
      "learning rate: 0.03125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:66\n",
      "learning rate: 0.015625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:67\n",
      "learning rate: 0.0078125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:68\n",
      "learning rate: 0.00390625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:69\n",
      "learning rate: 0.001953125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:70\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:71\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:72\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:73\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:74\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:75\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:76\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:77\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:78\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:79\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:80\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:81\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:82\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:83\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:84\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:85\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:86\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:87\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:88\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:89\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:90\n",
      "learning rate: 1.0\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:91\n",
      "learning rate: 0.5\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:92\n",
      "learning rate: 0.25\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:93\n",
      "learning rate: 0.125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:94\n",
      "learning rate: 0.0625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:95\n",
      "learning rate: 0.03125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:96\n",
      "learning rate: 0.015625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:97\n",
      "learning rate: 0.0078125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:98\n",
      "learning rate: 0.00390625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:99\n",
      "learning rate: 0.001953125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:100\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:101\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:102\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:103\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:104\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:105\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:106\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:107\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:108\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:109\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:110\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:111\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:112\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:113\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:114\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:115\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:116\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:117\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:118\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:119\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:120\n",
      "learning rate: 1.0\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:121\n",
      "learning rate: 0.5\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:122\n",
      "learning rate: 0.25\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:123\n",
      "learning rate: 0.125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:124\n",
      "learning rate: 0.0625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:125\n",
      "learning rate: 0.03125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:126\n",
      "learning rate: 0.015625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:127\n",
      "learning rate: 0.0078125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:128\n",
      "learning rate: 0.00390625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:129\n",
      "learning rate: 0.001953125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:130\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:131\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:132\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:133\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:134\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:135\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:136\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:137\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:138\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:139\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:140\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:141\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:142\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:143\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:144\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:145\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:146\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:147\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:148\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:149\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:150\n",
      "learning rate: 1.0\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:151\n",
      "learning rate: 0.5\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:152\n",
      "learning rate: 0.25\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:153\n",
      "learning rate: 0.125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:154\n",
      "learning rate: 0.0625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:155\n",
      "learning rate: 0.03125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:156\n",
      "learning rate: 0.015625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:157\n",
      "learning rate: 0.0078125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:158\n",
      "learning rate: 0.00390625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:159\n",
      "learning rate: 0.001953125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:160\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:161\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:162\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:163\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:164\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:165\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:166\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:167\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:168\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:169\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:170\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:171\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:172\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:173\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:174\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:175\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:176\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:177\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:178\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:179\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:180\n",
      "learning rate: 1.0\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:181\n",
      "learning rate: 0.5\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:182\n",
      "learning rate: 0.25\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:183\n",
      "learning rate: 0.125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:184\n",
      "learning rate: 0.0625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:185\n",
      "learning rate: 0.03125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:186\n",
      "learning rate: 0.015625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:187\n",
      "learning rate: 0.0078125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:188\n",
      "learning rate: 0.00390625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:189\n",
      "learning rate: 0.001953125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:190\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:191\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:192\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:193\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:194\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:195\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:196\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:197\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:198\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:199\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:200\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:201\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:202\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:203\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:204\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:205\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:206\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:207\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:208\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:209\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:210\n",
      "learning rate: 1.0\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:211\n",
      "learning rate: 0.5\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:212\n",
      "learning rate: 0.25\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:213\n",
      "learning rate: 0.125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:214\n",
      "learning rate: 0.0625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:215\n",
      "learning rate: 0.03125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:216\n",
      "learning rate: 0.015625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:217\n",
      "learning rate: 0.0078125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:218\n",
      "learning rate: 0.00390625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:219\n",
      "learning rate: 0.001953125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:220\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:221\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:222\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:223\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:224\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:225\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:226\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:227\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:228\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:229\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:230\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:231\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:232\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:233\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:234\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:235\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:236\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:237\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:238\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:239\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:240\n",
      "learning rate: 1.0\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:241\n",
      "learning rate: 0.5\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:242\n",
      "learning rate: 0.25\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:243\n",
      "learning rate: 0.125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:244\n",
      "learning rate: 0.0625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:245\n",
      "learning rate: 0.03125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:246\n",
      "learning rate: 0.015625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:247\n",
      "learning rate: 0.0078125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:248\n",
      "learning rate: 0.00390625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:249\n",
      "learning rate: 0.001953125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:250\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:251\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:252\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:253\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:254\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:255\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:256\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:257\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:258\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:259\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:260\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:261\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:262\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:263\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:264\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:265\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:266\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:267\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:268\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:269\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:270\n",
      "learning rate: 1.0\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:271\n",
      "learning rate: 0.5\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:272\n",
      "learning rate: 0.25\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:273\n",
      "learning rate: 0.125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:274\n",
      "learning rate: 0.0625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:275\n",
      "learning rate: 0.03125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:276\n",
      "learning rate: 0.015625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:277\n",
      "learning rate: 0.0078125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:278\n",
      "learning rate: 0.00390625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:279\n",
      "learning rate: 0.001953125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:280\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:281\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:282\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:283\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:284\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:285\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:286\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:287\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:288\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:289\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:290\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:291\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:292\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:293\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:294\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:295\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:296\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:297\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:298\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:299\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:300\n",
      "learning rate: 1.0\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:301\n",
      "learning rate: 0.5\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:302\n",
      "learning rate: 0.25\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:303\n",
      "learning rate: 0.125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:304\n",
      "learning rate: 0.0625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:305\n",
      "learning rate: 0.03125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:306\n",
      "learning rate: 0.015625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:307\n",
      "learning rate: 0.0078125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:308\n",
      "learning rate: 0.00390625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:309\n",
      "learning rate: 0.001953125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:310\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:311\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:312\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:313\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:314\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:315\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:316\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:317\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:318\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:319\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:320\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:321\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:322\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:323\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:324\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:325\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:326\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:327\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:328\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:329\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:330\n",
      "learning rate: 1.0\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:331\n",
      "learning rate: 0.5\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:332\n",
      "learning rate: 0.25\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:333\n",
      "learning rate: 0.125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:334\n",
      "learning rate: 0.0625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:335\n",
      "learning rate: 0.03125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:336\n",
      "learning rate: 0.015625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:337\n",
      "learning rate: 0.0078125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:338\n",
      "learning rate: 0.00390625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:339\n",
      "learning rate: 0.001953125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:340\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:341\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:342\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:343\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:344\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:345\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:346\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:347\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:348\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:349\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:350\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:351\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:352\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:353\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:354\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:355\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:356\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:357\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:358\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:359\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:360\n",
      "learning rate: 1.0\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:361\n",
      "learning rate: 0.5\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:362\n",
      "learning rate: 0.25\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:363\n",
      "learning rate: 0.125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:364\n",
      "learning rate: 0.0625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:365\n",
      "learning rate: 0.03125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:366\n",
      "learning rate: 0.015625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:367\n",
      "learning rate: 0.0078125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:368\n",
      "learning rate: 0.00390625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:369\n",
      "learning rate: 0.001953125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:370\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:371\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:372\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:373\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:374\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:375\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:376\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:377\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:378\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:379\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:380\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:381\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:382\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:383\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:384\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:385\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:386\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:387\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:388\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:389\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:390\n",
      "learning rate: 1.0\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:391\n",
      "learning rate: 0.5\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:392\n",
      "learning rate: 0.25\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:393\n",
      "learning rate: 0.125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:394\n",
      "learning rate: 0.0625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:395\n",
      "learning rate: 0.03125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:396\n",
      "learning rate: 0.015625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:397\n",
      "learning rate: 0.0078125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:398\n",
      "learning rate: 0.00390625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:399\n",
      "learning rate: 0.001953125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:400\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:401\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:402\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:403\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:404\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:405\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:406\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:407\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:408\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:409\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:410\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:411\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:412\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:413\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:414\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:415\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:416\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:417\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:418\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:419\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:420\n",
      "learning rate: 1.0\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:421\n",
      "learning rate: 0.5\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:422\n",
      "learning rate: 0.25\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:423\n",
      "learning rate: 0.125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:424\n",
      "learning rate: 0.0625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:425\n",
      "learning rate: 0.03125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:426\n",
      "learning rate: 0.015625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:427\n",
      "learning rate: 0.0078125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:428\n",
      "learning rate: 0.00390625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:429\n",
      "learning rate: 0.001953125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:430\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:431\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:432\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:433\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:434\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:435\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:436\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:437\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:438\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:439\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:440\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:441\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:442\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:443\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:444\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:445\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:446\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:447\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:448\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:449\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:450\n",
      "learning rate: 1.0\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:451\n",
      "learning rate: 0.5\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:452\n",
      "learning rate: 0.25\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:453\n",
      "learning rate: 0.125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:454\n",
      "learning rate: 0.0625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:455\n",
      "learning rate: 0.03125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:456\n",
      "learning rate: 0.015625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:457\n",
      "learning rate: 0.0078125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:458\n",
      "learning rate: 0.00390625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:459\n",
      "learning rate: 0.001953125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:460\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:461\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:462\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:463\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:464\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:465\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:466\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:467\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:468\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:469\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:470\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:471\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:472\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:473\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:474\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:475\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:476\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:477\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:478\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:479\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:480\n",
      "learning rate: 1.0\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:481\n",
      "learning rate: 0.5\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:482\n",
      "learning rate: 0.25\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:483\n",
      "learning rate: 0.125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:484\n",
      "learning rate: 0.0625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:485\n",
      "learning rate: 0.03125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:486\n",
      "learning rate: 0.015625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:487\n",
      "learning rate: 0.0078125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:488\n",
      "learning rate: 0.00390625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:489\n",
      "learning rate: 0.001953125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:490\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:491\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:492\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:493\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:494\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:495\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:496\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:497\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:498\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Previous best:0.9868\n",
      "step size index:499\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.029352  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 1.019963 \n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "best_model = NeuralNetwork().to(device)\n",
    "\n",
    "best_model.apply(weights_init)\n",
    "model_4 =test_train(0,end_epoch,0,0,0,best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a0b61189-538e-411d-8263-b452b661b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_4.state_dict(), \"model_4.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f431b528-9349-4eb2-9810-e2231ba34dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "Previous best:0\n",
      "step size index:0\n",
      "learning rate: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flackjacket/miniconda3/envs/machine-learning-env/lib/python3.9/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([500, 1])) that is different to the input size (torch.Size([500, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:0\n",
      "learning rate: 1.0\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:1\n",
      "learning rate: 0.5\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:2\n",
      "learning rate: 0.25\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:3\n",
      "learning rate: 0.125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:4\n",
      "learning rate: 0.0625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:5\n",
      "learning rate: 0.03125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:6\n",
      "learning rate: 0.015625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:7\n",
      "learning rate: 0.0078125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:8\n",
      "learning rate: 0.00390625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:9\n",
      "learning rate: 0.001953125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:10\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:11\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:12\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:13\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:14\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:15\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:16\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:17\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:18\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:19\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:20\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:21\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:22\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:23\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:24\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:25\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:26\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:27\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:28\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:29\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:30\n",
      "learning rate: 1.0\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:31\n",
      "learning rate: 0.5\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:32\n",
      "learning rate: 0.25\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:33\n",
      "learning rate: 0.125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:34\n",
      "learning rate: 0.0625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:35\n",
      "learning rate: 0.03125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:36\n",
      "learning rate: 0.015625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:37\n",
      "learning rate: 0.0078125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:38\n",
      "learning rate: 0.00390625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:39\n",
      "learning rate: 0.001953125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:40\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:41\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:42\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:43\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:44\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:45\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:46\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:47\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:48\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:49\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:50\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:51\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:52\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:53\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:54\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:55\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:56\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:57\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:58\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:59\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:60\n",
      "learning rate: 1.0\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:61\n",
      "learning rate: 0.5\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:62\n",
      "learning rate: 0.25\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:63\n",
      "learning rate: 0.125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:64\n",
      "learning rate: 0.0625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:65\n",
      "learning rate: 0.03125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:66\n",
      "learning rate: 0.015625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:67\n",
      "learning rate: 0.0078125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:68\n",
      "learning rate: 0.00390625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:69\n",
      "learning rate: 0.001953125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:70\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:71\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:72\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:73\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:74\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:75\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:76\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:77\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:78\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:79\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:80\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:81\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:82\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:83\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:84\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:85\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:86\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:87\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:88\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:89\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:90\n",
      "learning rate: 1.0\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:91\n",
      "learning rate: 0.5\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:92\n",
      "learning rate: 0.25\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:93\n",
      "learning rate: 0.125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:94\n",
      "learning rate: 0.0625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:95\n",
      "learning rate: 0.03125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:96\n",
      "learning rate: 0.015625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:97\n",
      "learning rate: 0.0078125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:98\n",
      "learning rate: 0.00390625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:99\n",
      "learning rate: 0.001953125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:100\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:101\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:102\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:103\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:104\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:105\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:106\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:107\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:108\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:109\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:110\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:111\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:112\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:113\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:114\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:115\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:116\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:117\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:118\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:119\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:120\n",
      "learning rate: 1.0\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:121\n",
      "learning rate: 0.5\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:122\n",
      "learning rate: 0.25\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:123\n",
      "learning rate: 0.125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:124\n",
      "learning rate: 0.0625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:125\n",
      "learning rate: 0.03125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:126\n",
      "learning rate: 0.015625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:127\n",
      "learning rate: 0.0078125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:128\n",
      "learning rate: 0.00390625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:129\n",
      "learning rate: 0.001953125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:130\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:131\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:132\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:133\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:134\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:135\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:136\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:137\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:138\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:139\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:140\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:141\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:142\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:143\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:144\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:145\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:146\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:147\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:148\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:149\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:150\n",
      "learning rate: 1.0\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:151\n",
      "learning rate: 0.5\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:152\n",
      "learning rate: 0.25\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:153\n",
      "learning rate: 0.125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:154\n",
      "learning rate: 0.0625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:155\n",
      "learning rate: 0.03125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:156\n",
      "learning rate: 0.015625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:157\n",
      "learning rate: 0.0078125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:158\n",
      "learning rate: 0.00390625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:159\n",
      "learning rate: 0.001953125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:160\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:161\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:162\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:163\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:164\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:165\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:166\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:167\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:168\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:169\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:170\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:171\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:172\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:173\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:174\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:175\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:176\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:177\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:178\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:179\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:180\n",
      "learning rate: 1.0\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:181\n",
      "learning rate: 0.5\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:182\n",
      "learning rate: 0.25\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:183\n",
      "learning rate: 0.125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:184\n",
      "learning rate: 0.0625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:185\n",
      "learning rate: 0.03125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:186\n",
      "learning rate: 0.015625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:187\n",
      "learning rate: 0.0078125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:188\n",
      "learning rate: 0.00390625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:189\n",
      "learning rate: 0.001953125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:190\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:191\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:192\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:193\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:194\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:195\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:196\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:197\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:198\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:199\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:200\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:201\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:202\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:203\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:204\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:205\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:206\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:207\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:208\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:209\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:210\n",
      "learning rate: 1.0\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:211\n",
      "learning rate: 0.5\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:212\n",
      "learning rate: 0.25\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:213\n",
      "learning rate: 0.125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:214\n",
      "learning rate: 0.0625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:215\n",
      "learning rate: 0.03125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:216\n",
      "learning rate: 0.015625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:217\n",
      "learning rate: 0.0078125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:218\n",
      "learning rate: 0.00390625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:219\n",
      "learning rate: 0.001953125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:220\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:221\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:222\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:223\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:224\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:225\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:226\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:227\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:228\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:229\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:230\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:231\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:232\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:233\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:234\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:235\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:236\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:237\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:238\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:239\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:240\n",
      "learning rate: 1.0\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:241\n",
      "learning rate: 0.5\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:242\n",
      "learning rate: 0.25\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:243\n",
      "learning rate: 0.125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:244\n",
      "learning rate: 0.0625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:245\n",
      "learning rate: 0.03125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:246\n",
      "learning rate: 0.015625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:247\n",
      "learning rate: 0.0078125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:248\n",
      "learning rate: 0.00390625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:249\n",
      "learning rate: 0.001953125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:250\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:251\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:252\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:253\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:254\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:255\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:256\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:257\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:258\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:259\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:260\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:261\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:262\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:263\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:264\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:265\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:266\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:267\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:268\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:269\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:270\n",
      "learning rate: 1.0\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:271\n",
      "learning rate: 0.5\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:272\n",
      "learning rate: 0.25\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:273\n",
      "learning rate: 0.125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:274\n",
      "learning rate: 0.0625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:275\n",
      "learning rate: 0.03125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:276\n",
      "learning rate: 0.015625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:277\n",
      "learning rate: 0.0078125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:278\n",
      "learning rate: 0.00390625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:279\n",
      "learning rate: 0.001953125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:280\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:281\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:282\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:283\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:284\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:285\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:286\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:287\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:288\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:289\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:290\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:291\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:292\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:293\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:294\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:295\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:296\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:297\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:298\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:299\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:300\n",
      "learning rate: 1.0\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:301\n",
      "learning rate: 0.5\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:302\n",
      "learning rate: 0.25\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:303\n",
      "learning rate: 0.125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:304\n",
      "learning rate: 0.0625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:305\n",
      "learning rate: 0.03125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:306\n",
      "learning rate: 0.015625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:307\n",
      "learning rate: 0.0078125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:308\n",
      "learning rate: 0.00390625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:309\n",
      "learning rate: 0.001953125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:310\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:311\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:312\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:313\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:314\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:315\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:316\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:317\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:318\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:319\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:320\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:321\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:322\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:323\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:324\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:325\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:326\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:327\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:328\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:329\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:330\n",
      "learning rate: 1.0\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:331\n",
      "learning rate: 0.5\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:332\n",
      "learning rate: 0.25\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:333\n",
      "learning rate: 0.125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:334\n",
      "learning rate: 0.0625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:335\n",
      "learning rate: 0.03125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:336\n",
      "learning rate: 0.015625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:337\n",
      "learning rate: 0.0078125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:338\n",
      "learning rate: 0.00390625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:339\n",
      "learning rate: 0.001953125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:340\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:341\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:342\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:343\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:344\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:345\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:346\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:347\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:348\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:349\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:350\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:351\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:352\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:353\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:354\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:355\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:356\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:357\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:358\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:359\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:360\n",
      "learning rate: 1.0\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:361\n",
      "learning rate: 0.5\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:362\n",
      "learning rate: 0.25\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:363\n",
      "learning rate: 0.125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:364\n",
      "learning rate: 0.0625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:365\n",
      "learning rate: 0.03125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:366\n",
      "learning rate: 0.015625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:367\n",
      "learning rate: 0.0078125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:368\n",
      "learning rate: 0.00390625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:369\n",
      "learning rate: 0.001953125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:370\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:371\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:372\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:373\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:374\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:375\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:376\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:377\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:378\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:379\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:380\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:381\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:382\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:383\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:384\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:385\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:386\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:387\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:388\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:389\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:390\n",
      "learning rate: 1.0\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:391\n",
      "learning rate: 0.5\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:392\n",
      "learning rate: 0.25\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:393\n",
      "learning rate: 0.125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:394\n",
      "learning rate: 0.0625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:395\n",
      "learning rate: 0.03125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:396\n",
      "learning rate: 0.015625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:397\n",
      "learning rate: 0.0078125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:398\n",
      "learning rate: 0.00390625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:399\n",
      "learning rate: 0.001953125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:400\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:401\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:402\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:403\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:404\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:405\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:406\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:407\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:408\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:409\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:410\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:411\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:412\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:413\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:414\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:415\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:416\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:417\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:418\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:419\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:420\n",
      "learning rate: 1.0\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:421\n",
      "learning rate: 0.5\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:422\n",
      "learning rate: 0.25\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:423\n",
      "learning rate: 0.125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:424\n",
      "learning rate: 0.0625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:425\n",
      "learning rate: 0.03125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:426\n",
      "learning rate: 0.015625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:427\n",
      "learning rate: 0.0078125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:428\n",
      "learning rate: 0.00390625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:429\n",
      "learning rate: 0.001953125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:430\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:431\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:432\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:433\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:434\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:435\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:436\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:437\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:438\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:439\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:440\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:441\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:442\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:443\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:444\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:445\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:446\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:447\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:448\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:449\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:450\n",
      "learning rate: 1.0\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:451\n",
      "learning rate: 0.5\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:452\n",
      "learning rate: 0.25\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:453\n",
      "learning rate: 0.125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:454\n",
      "learning rate: 0.0625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:455\n",
      "learning rate: 0.03125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:456\n",
      "learning rate: 0.015625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:457\n",
      "learning rate: 0.0078125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:458\n",
      "learning rate: 0.00390625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:459\n",
      "learning rate: 0.001953125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:460\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:461\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:462\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:463\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:464\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:465\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:466\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:467\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:468\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:469\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:470\n",
      "learning rate: 9.5367431640625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:471\n",
      "learning rate: 4.76837158203125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:472\n",
      "learning rate: 2.384185791015625e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:473\n",
      "learning rate: 1.1920928955078125e-07\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:474\n",
      "learning rate: 5.960464477539063e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:475\n",
      "learning rate: 2.9802322387695312e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:476\n",
      "learning rate: 1.4901161193847656e-08\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:477\n",
      "learning rate: 7.450580596923828e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:478\n",
      "learning rate: 3.725290298461914e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:479\n",
      "learning rate: 1.862645149230957e-09\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:480\n",
      "learning rate: 1.0\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:481\n",
      "learning rate: 0.5\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:482\n",
      "learning rate: 0.25\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:483\n",
      "learning rate: 0.125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:484\n",
      "learning rate: 0.0625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:485\n",
      "learning rate: 0.03125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:486\n",
      "learning rate: 0.015625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:487\n",
      "learning rate: 0.0078125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:488\n",
      "learning rate: 0.00390625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:489\n",
      "learning rate: 0.001953125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:490\n",
      "learning rate: 0.0009765625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:491\n",
      "learning rate: 0.00048828125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:492\n",
      "learning rate: 0.000244140625\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:493\n",
      "learning rate: 0.0001220703125\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:494\n",
      "learning rate: 6.103515625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:495\n",
      "learning rate: 3.0517578125e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:496\n",
      "learning rate: 1.52587890625e-05\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:497\n",
      "learning rate: 7.62939453125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:498\n",
      "learning rate: 3.814697265625e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Previous best:0.9896\n",
      "step size index:499\n",
      "learning rate: 1.9073486328125e-06\n",
      "loss: 1.024742  [    0/ 2500]\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "best_model = NeuralNetwork().to(device)\n",
    "\n",
    "best_model.apply(weights_init)\n",
    "model_5 =test_train(0,end_epoch,0,0,0,best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe8cda-2d21-43e4-98d0-0254c49db643",
   "metadata": {},
   "source": [
    "OK well for whatever reason, it looks like I lucked out!\n",
    "\n",
    "To reuse the minesweeper analogy, looks like our unlucky soldier landed on an untouched patch of grass with nymphs, rose water, and coconuts, and smartly figured out that everything else is a battlefield. It looks like he doesn't want to take even a tiny step in any direction. \n",
    "\n",
    "There might be a bug somewhere giving him depression, but who knows?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d564b140-461c-4078-8e07-9ba4ddb6e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_5.state_dict(), \"model_5.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0650a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for param_tensor in model.state_dict():\n",
    "#     print(param_tensor, \"\\t\", model.state_dict()[param_tensor])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1d02be-8da3-4812-b7d5-0e2ab581ae81",
   "metadata": {},
   "source": [
    "We can take a look at what the tensor list looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a79a6516",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list = [tensor for tensor in model_1.state_dict()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fb0c8f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linear_relu_stack.0.weight',\n",
       " 'linear_relu_stack.0.bias',\n",
       " 'linear_relu_stack.2.weight',\n",
       " 'linear_relu_stack.2.bias',\n",
       " 'linear_relu_stack.4.weight',\n",
       " 'linear_relu_stack.4.bias']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f5fdab-21ad-4120-b61a-c0383be1f294",
   "metadata": {},
   "source": [
    "OK so now we have two models. Model_1 and model_5.\n",
    "\n",
    "The first has so-so accuracy and average error, and the second has .\n",
    "\n",
    "Can we do better?\n",
    "Yes.  Let's write a function that will give us weights somewhere in between the two extremes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d853cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = NeuralNetwork().to(device)\n",
    "\n",
    "def linear_combo_dic(model_a,model_b,t):\n",
    "    od = OrderedDict()\n",
    "    for tensor in model_a.state_dict():\n",
    "        od[tensor]= model_a.state_dict()[tensor]*t + (1-t)*model_b.state_dict()[tensor]\n",
    "    return od\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf716c9-1c4d-48e5-8757-c5c5af681922",
   "metadata": {},
   "source": [
    "^ TLDR: Given two matrices of weights, A and B, and a number t in [0,1], \n",
    "\n",
    "the function above returns A*t + (1-t)*B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38882e72-32e6-457c-9e5e-87e246a3288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now that we have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca18479a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.022875 \n",
      "\n",
      "0.001001001001001001\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.020127 \n",
      "\n",
      "0.002002002002002002\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.017348 \n",
      "\n",
      "0.003003003003003003\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.014608 \n",
      "\n",
      "0.004004004004004004\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.011890 \n",
      "\n",
      "0.005005005005005005\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.009105 \n",
      "\n",
      "0.006006006006006006\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.006418 \n",
      "\n",
      "0.007007007007007007\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.003655 \n",
      "\n",
      "0.008008008008008008\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 1.000944 \n",
      "\n",
      "0.009009009009009009\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.998048 \n",
      "\n",
      "0.01001001001001001\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.995375 \n",
      "\n",
      "0.011011011011011011\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.992610 \n",
      "\n",
      "0.012012012012012012\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.989791 \n",
      "\n",
      "0.013013013013013013\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.987006 \n",
      "\n",
      "0.014014014014014014\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.984269 \n",
      "\n",
      "0.015015015015015015\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.981438 \n",
      "\n",
      "0.016016016016016016\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.978583 \n",
      "\n",
      "0.017017017017017015\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.975854 \n",
      "\n",
      "0.018018018018018018\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.973067 \n",
      "\n",
      "0.01901901901901902\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.970216 \n",
      "\n",
      "0.02002002002002002\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.967386 \n",
      "\n",
      "0.02102102102102102\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.964643 \n",
      "\n",
      "0.022022022022022022\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.961743 \n",
      "\n",
      "0.023023023023023025\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.958918 \n",
      "\n",
      "0.024024024024024024\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.956107 \n",
      "\n",
      "0.025025025025025023\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.953319 \n",
      "\n",
      "0.026026026026026026\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.950433 \n",
      "\n",
      "0.02702702702702703\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.947567 \n",
      "\n",
      "0.028028028028028028\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.944790 \n",
      "\n",
      "0.029029029029029027\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.941914 \n",
      "\n",
      "0.03003003003003003\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.939022 \n",
      "\n",
      "0.031031031031031032\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.936174 \n",
      "\n",
      "0.03203203203203203\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.933345 \n",
      "\n",
      "0.03303303303303303\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.930455 \n",
      "\n",
      "0.03403403403403403\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.927528 \n",
      "\n",
      "0.035035035035035036\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.924702 \n",
      "\n",
      "0.036036036036036036\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.921858 \n",
      "\n",
      "0.037037037037037035\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.918985 \n",
      "\n",
      "0.03803803803803804\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.916084 \n",
      "\n",
      "0.03903903903903904\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.913208 \n",
      "\n",
      "0.04004004004004004\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.910306 \n",
      "\n",
      "0.04104104104104104\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.907420 \n",
      "\n",
      "0.04204204204204204\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.904500 \n",
      "\n",
      "0.043043043043043044\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.901622 \n",
      "\n",
      "0.044044044044044044\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.898753 \n",
      "\n",
      "0.04504504504504504\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.895871 \n",
      "\n",
      "0.04604604604604605\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.892906 \n",
      "\n",
      "0.04704704704704705\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.890020 \n",
      "\n",
      "0.04804804804804805\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.887069 \n",
      "\n",
      "0.04904904904904905\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.884207 \n",
      "\n",
      "0.050050050050050046\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.881301 \n",
      "\n",
      "0.05105105105105105\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.878377 \n",
      "\n",
      "0.05205205205205205\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.875402 \n",
      "\n",
      "0.05305305305305305\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.872466 \n",
      "\n",
      "0.05405405405405406\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.869610 \n",
      "\n",
      "0.055055055055055056\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.866595 \n",
      "\n",
      "0.056056056056056056\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.863678 \n",
      "\n",
      "0.057057057057057055\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.860732 \n",
      "\n",
      "0.058058058058058054\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.857865 \n",
      "\n",
      "0.05905905905905906\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.854888 \n",
      "\n",
      "0.06006006006006006\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.851937 \n",
      "\n",
      "0.06106106106106106\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.849043 \n",
      "\n",
      "0.062062062062062065\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.846043 \n",
      "\n",
      "0.06306306306306306\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.843077 \n",
      "\n",
      "0.06406406406406406\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.840133 \n",
      "\n",
      "0.06506506506506507\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.837248 \n",
      "\n",
      "0.06606606606606606\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.834263 \n",
      "\n",
      "0.06706706706706707\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.831305 \n",
      "\n",
      "0.06806806806806806\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.828320 \n",
      "\n",
      "0.06906906906906907\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.825423 \n",
      "\n",
      "0.07007007007007007\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.822376 \n",
      "\n",
      "0.07107107107107107\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.819426 \n",
      "\n",
      "0.07207207207207207\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.816481 \n",
      "\n",
      "0.07307307307307308\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.813480 \n",
      "\n",
      "0.07407407407407407\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.810529 \n",
      "\n",
      "0.07507507507507508\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.807600 \n",
      "\n",
      "0.07607607607607608\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.804672 \n",
      "\n",
      "0.07707707707707707\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.801639 \n",
      "\n",
      "0.07807807807807808\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.798674 \n",
      "\n",
      "0.07907907907907907\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.795709 \n",
      "\n",
      "0.08008008008008008\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.792686 \n",
      "\n",
      "0.08108108108108109\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.789763 \n",
      "\n",
      "0.08208208208208208\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.786817 \n",
      "\n",
      "0.08308308308308308\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.783842 \n",
      "\n",
      "0.08408408408408408\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.780827 \n",
      "\n",
      "0.08508508508508508\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.777850 \n",
      "\n",
      "0.08608608608608609\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.774882 \n",
      "\n",
      "0.08708708708708708\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.771856 \n",
      "\n",
      "0.08808808808808809\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.768906 \n",
      "\n",
      "0.0890890890890891\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.765955 \n",
      "\n",
      "0.09009009009009009\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.762987 \n",
      "\n",
      "0.09109109109109109\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.759982 \n",
      "\n",
      "0.0920920920920921\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.757001 \n",
      "\n",
      "0.09309309309309309\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.754026 \n",
      "\n",
      "0.0940940940940941\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.751059 \n",
      "\n",
      "0.09509509509509509\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.748038 \n",
      "\n",
      "0.0960960960960961\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.745061 \n",
      "\n",
      "0.0970970970970971\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.742107 \n",
      "\n",
      "0.0980980980980981\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.739120 \n",
      "\n",
      "0.0990990990990991\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.736128 \n",
      "\n",
      "0.10010010010010009\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.733157 \n",
      "\n",
      "0.1011011011011011\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.730173 \n",
      "\n",
      "0.1021021021021021\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.727159 \n",
      "\n",
      "0.1031031031031031\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.724187 \n",
      "\n",
      "0.1041041041041041\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.721245 \n",
      "\n",
      "0.10510510510510511\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.718252 \n",
      "\n",
      "0.1061061061061061\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.715284 \n",
      "\n",
      "0.10710710710710711\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.712309 \n",
      "\n",
      "0.10810810810810811\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.709315 \n",
      "\n",
      "0.1091091091091091\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.706314 \n",
      "\n",
      "0.11011011011011011\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.703355 \n",
      "\n",
      "0.1111111111111111\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.700418 \n",
      "\n",
      "0.11211211211211211\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.697422 \n",
      "\n",
      "0.11311311311311312\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.694459 \n",
      "\n",
      "0.11411411411411411\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.691487 \n",
      "\n",
      "0.11511511511511512\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.688487 \n",
      "\n",
      "0.11611611611611611\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.685552 \n",
      "\n",
      "0.11711711711711711\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.682564 \n",
      "\n",
      "0.11811811811811812\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.679585 \n",
      "\n",
      "0.11911911911911911\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.676668 \n",
      "\n",
      "0.12012012012012012\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.673705 \n",
      "\n",
      "0.12112112112112113\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.670681 \n",
      "\n",
      "0.12212212212212212\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.667745 \n",
      "\n",
      "0.12312312312312312\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.664788 \n",
      "\n",
      "0.12412412412412413\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.661860 \n",
      "\n",
      "0.12512512512512514\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.658849 \n",
      "\n",
      "0.12612612612612611\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.655951 \n",
      "\n",
      "0.12712712712712712\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.652984 \n",
      "\n",
      "0.12812812812812813\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.650043 \n",
      "\n",
      "0.12912912912912913\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.647106 \n",
      "\n",
      "0.13013013013013014\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.644118 \n",
      "\n",
      "0.13113113113113112\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.641199 \n",
      "\n",
      "0.13213213213213212\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.638259 \n",
      "\n",
      "0.13313313313313313\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.635306 \n",
      "\n",
      "0.13413413413413414\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.632398 \n",
      "\n",
      "0.13513513513513514\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.629451 \n",
      "\n",
      "0.13613613613613612\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.626548 \n",
      "\n",
      "0.13713713713713713\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.623583 \n",
      "\n",
      "0.13813813813813813\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.620674 \n",
      "\n",
      "0.13913913913913914\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.617701 \n",
      "\n",
      "0.14014014014014015\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.614818 \n",
      "\n",
      "0.14114114114114115\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.611919 \n",
      "\n",
      "0.14214214214214213\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.609006 \n",
      "\n",
      "0.14314314314314314\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.606107 \n",
      "\n",
      "0.14414414414414414\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.603213 \n",
      "\n",
      "0.14514514514514515\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.600263 \n",
      "\n",
      "0.14614614614614616\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.597374 \n",
      "\n",
      "0.14714714714714713\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.594452 \n",
      "\n",
      "0.14814814814814814\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.591545 \n",
      "\n",
      "0.14914914914914915\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.588701 \n",
      "\n",
      "0.15015015015015015\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.585794 \n",
      "\n",
      "0.15115115115115116\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.582941 \n",
      "\n",
      "0.15215215215215216\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.580007 \n",
      "\n",
      "0.15315315315315314\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.577154 \n",
      "\n",
      "0.15415415415415415\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.574278 \n",
      "\n",
      "0.15515515515515516\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.571372 \n",
      "\n",
      "0.15615615615615616\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.568570 \n",
      "\n",
      "0.15715715715715717\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.565687 \n",
      "\n",
      "0.15815815815815815\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.562846 \n",
      "\n",
      "0.15915915915915915\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.559978 \n",
      "\n",
      "0.16016016016016016\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.557136 \n",
      "\n",
      "0.16116116116116116\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.554285 \n",
      "\n",
      "0.16216216216216217\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.551409 \n",
      "\n",
      "0.16316316316316315\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.548597 \n",
      "\n",
      "0.16416416416416416\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.545781 \n",
      "\n",
      "0.16516516516516516\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.542961 \n",
      "\n",
      "0.16616616616616617\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.540089 \n",
      "\n",
      "0.16716716716716717\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.537348 \n",
      "\n",
      "0.16816816816816815\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.534500 \n",
      "\n",
      "0.16916916916916916\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.531666 \n",
      "\n",
      "0.17017017017017017\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.528832 \n",
      "\n",
      "0.17117117117117117\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.526117 \n",
      "\n",
      "0.17217217217217218\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.523261 \n",
      "\n",
      "0.17317317317317318\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.520488 \n",
      "\n",
      "0.17417417417417416\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.517667 \n",
      "\n",
      "0.17517517517517517\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.514953 \n",
      "\n",
      "0.17617617617617617\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.512143 \n",
      "\n",
      "0.17717717717717718\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.509352 \n",
      "\n",
      "0.1781781781781782\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.506600 \n",
      "\n",
      "0.17917917917917917\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.503869 \n",
      "\n",
      "0.18018018018018017\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.501103 \n",
      "\n",
      "0.18118118118118118\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.498341 \n",
      "\n",
      "0.18218218218218218\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.495602 \n",
      "\n",
      "0.1831831831831832\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.492870 \n",
      "\n",
      "0.1841841841841842\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.490118 \n",
      "\n",
      "0.18518518518518517\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.487381 \n",
      "\n",
      "0.18618618618618618\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.484692 \n",
      "\n",
      "0.1871871871871872\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.481952 \n",
      "\n",
      "0.1881881881881882\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.479249 \n",
      "\n",
      "0.1891891891891892\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.476557 \n",
      "\n",
      "0.19019019019019018\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.473857 \n",
      "\n",
      "0.19119119119119118\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.471164 \n",
      "\n",
      "0.1921921921921922\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.468463 \n",
      "\n",
      "0.1931931931931932\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.465790 \n",
      "\n",
      "0.1941941941941942\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.463107 \n",
      "\n",
      "0.19519519519519518\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.460474 \n",
      "\n",
      "0.1961961961961962\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.457775 \n",
      "\n",
      "0.1971971971971972\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.455134 \n",
      "\n",
      "0.1981981981981982\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.452477 \n",
      "\n",
      "0.1991991991991992\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.449860 \n",
      "\n",
      "0.20020020020020018\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.447221 \n",
      "\n",
      "0.2012012012012012\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.444605 \n",
      "\n",
      "0.2022022022022022\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.441976 \n",
      "\n",
      "0.2032032032032032\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.439364 \n",
      "\n",
      "0.2042042042042042\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.436758 \n",
      "\n",
      "0.20520520520520522\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.434141 \n",
      "\n",
      "0.2062062062062062\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.431558 \n",
      "\n",
      "0.2072072072072072\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.428972 \n",
      "\n",
      "0.2082082082082082\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.426401 \n",
      "\n",
      "0.2092092092092092\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.423796 \n",
      "\n",
      "0.21021021021021022\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.421232 \n",
      "\n",
      "0.2112112112112112\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.418690 \n",
      "\n",
      "0.2122122122122122\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.416145 \n",
      "\n",
      "0.2132132132132132\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.413565 \n",
      "\n",
      "0.21421421421421422\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.411029 \n",
      "\n",
      "0.21521521521521522\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.408510 \n",
      "\n",
      "0.21621621621621623\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.406018 \n",
      "\n",
      "0.2172172172172172\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.403462 \n",
      "\n",
      "0.2182182182182182\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.400984 \n",
      "\n",
      "0.21921921921921922\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.398467 \n",
      "\n",
      "0.22022022022022023\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.395961 \n",
      "\n",
      "0.22122122122122123\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.393486 \n",
      "\n",
      "0.2222222222222222\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.391053 \n",
      "\n",
      "0.22322322322322322\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.388573 \n",
      "\n",
      "0.22422422422422422\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.386102 \n",
      "\n",
      "0.22522522522522523\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.383648 \n",
      "\n",
      "0.22622622622622623\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.381195 \n",
      "\n",
      "0.2272272272272272\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.378744 \n",
      "\n",
      "0.22822822822822822\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.376316 \n",
      "\n",
      "0.22922922922922923\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.373915 \n",
      "\n",
      "0.23023023023023023\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.371476 \n",
      "\n",
      "0.23123123123123124\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.369067 \n",
      "\n",
      "0.23223223223223222\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.366684 \n",
      "\n",
      "0.23323323323323322\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.364320 \n",
      "\n",
      "0.23423423423423423\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.361921 \n",
      "\n",
      "0.23523523523523523\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.359538 \n",
      "\n",
      "0.23623623623623624\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.357169 \n",
      "\n",
      "0.23723723723723725\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.354799 \n",
      "\n",
      "0.23823823823823823\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.352462 \n",
      "\n",
      "0.23923923923923923\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.350130 \n",
      "\n",
      "0.24024024024024024\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.347837 \n",
      "\n",
      "0.24124124124124124\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.345477 \n",
      "\n",
      "0.24224224224224225\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.343165 \n",
      "\n",
      "0.24324324324324323\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.340872 \n",
      "\n",
      "0.24424424424424424\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.338565 \n",
      "\n",
      "0.24524524524524524\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.336290 \n",
      "\n",
      "0.24624624624624625\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.334029 \n",
      "\n",
      "0.24724724724724725\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.331751 \n",
      "\n",
      "0.24824824824824826\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.329462 \n",
      "\n",
      "0.24924924924924924\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.327231 \n",
      "\n",
      "0.2502502502502503\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.324976 \n",
      "\n",
      "0.25125125125125125\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.322748 \n",
      "\n",
      "0.25225225225225223\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.320499 \n",
      "\n",
      "0.25325325325325326\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.318300 \n",
      "\n",
      "0.25425425425425424\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.316086 \n",
      "\n",
      "0.2552552552552553\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.313899 \n",
      "\n",
      "0.25625625625625625\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.311707 \n",
      "\n",
      "0.25725725725725723\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.309517 \n",
      "\n",
      "0.25825825825825827\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.307341 \n",
      "\n",
      "0.25925925925925924\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.305193 \n",
      "\n",
      "0.2602602602602603\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.303046 \n",
      "\n",
      "0.26126126126126126\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.300897 \n",
      "\n",
      "0.26226226226226224\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.298745 \n",
      "\n",
      "0.26326326326326327\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.296646 \n",
      "\n",
      "0.26426426426426425\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.294541 \n",
      "\n",
      "0.2652652652652653\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.292449 \n",
      "\n",
      "0.26626626626626626\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.290320 \n",
      "\n",
      "0.26726726726726724\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.288244 \n",
      "\n",
      "0.2682682682682683\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.286148 \n",
      "\n",
      "0.26926926926926925\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.284086 \n",
      "\n",
      "0.2702702702702703\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.282023 \n",
      "\n",
      "0.27127127127127126\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.279972 \n",
      "\n",
      "0.27227227227227224\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.277934 \n",
      "\n",
      "0.2732732732732733\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.275901 \n",
      "\n",
      "0.27427427427427425\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.273887 \n",
      "\n",
      "0.2752752752752753\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.271863 \n",
      "\n",
      "0.27627627627627627\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.269875 \n",
      "\n",
      "0.2772772772772773\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.267887 \n",
      "\n",
      "0.2782782782782783\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.265911 \n",
      "\n",
      "0.27927927927927926\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.263927 \n",
      "\n",
      "0.2802802802802803\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.261960 \n",
      "\n",
      "0.28128128128128127\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.260000 \n",
      "\n",
      "0.2822822822822823\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.258061 \n",
      "\n",
      "0.2832832832832833\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.256126 \n",
      "\n",
      "0.28428428428428426\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.254211 \n",
      "\n",
      "0.2852852852852853\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.252308 \n",
      "\n",
      "0.2862862862862863\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.250399 \n",
      "\n",
      "0.2872872872872873\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.248506 \n",
      "\n",
      "0.2882882882882883\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.246602 \n",
      "\n",
      "0.28928928928928926\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.244727 \n",
      "\n",
      "0.2902902902902903\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.242856 \n",
      "\n",
      "0.2912912912912913\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.241009 \n",
      "\n",
      "0.2922922922922923\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.239153 \n",
      "\n",
      "0.2932932932932933\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.237324 \n",
      "\n",
      "0.29429429429429427\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.235508 \n",
      "\n",
      "0.2952952952952953\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.233679 \n",
      "\n",
      "0.2962962962962963\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.231871 \n",
      "\n",
      "0.2972972972972973\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.230085 \n",
      "\n",
      "0.2982982982982983\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.228297 \n",
      "\n",
      "0.29929929929929927\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.226541 \n",
      "\n",
      "0.3003003003003003\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.224754 \n",
      "\n",
      "0.3013013013013013\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.223002 \n",
      "\n",
      "0.3023023023023023\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.221235 \n",
      "\n",
      "0.3033033033033033\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.219514 \n",
      "\n",
      "0.30430430430430433\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.217782 \n",
      "\n",
      "0.3053053053053053\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.216066 \n",
      "\n",
      "0.3063063063063063\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.214363 \n",
      "\n",
      "0.3073073073073073\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.212674 \n",
      "\n",
      "0.3083083083083083\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.210973 \n",
      "\n",
      "0.30930930930930933\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.209288 \n",
      "\n",
      "0.3103103103103103\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.207615 \n",
      "\n",
      "0.3113113113113113\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.205966 \n",
      "\n",
      "0.3123123123123123\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.204294 \n",
      "\n",
      "0.3133133133133133\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.202666 \n",
      "\n",
      "0.31431431431431434\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.201045 \n",
      "\n",
      "0.3153153153153153\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.199439 \n",
      "\n",
      "0.3163163163163163\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.197825 \n",
      "\n",
      "0.3173173173173173\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.196236 \n",
      "\n",
      "0.3183183183183183\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.194650 \n",
      "\n",
      "0.31931931931931934\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.193042 \n",
      "\n",
      "0.3203203203203203\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.191493 \n",
      "\n",
      "0.3213213213213213\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.189931 \n",
      "\n",
      "0.32232232232232233\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.188391 \n",
      "\n",
      "0.3233233233233233\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.186824 \n",
      "\n",
      "0.32432432432432434\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.185301 \n",
      "\n",
      "0.3253253253253253\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.183805 \n",
      "\n",
      "0.3263263263263263\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.182287 \n",
      "\n",
      "0.32732732732732733\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.180759 \n",
      "\n",
      "0.3283283283283283\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.179275 \n",
      "\n",
      "0.32932932932932935\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.177786 \n",
      "\n",
      "0.3303303303303303\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.176315 \n",
      "\n",
      "0.3313313313313313\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.174843 \n",
      "\n",
      "0.33233233233233234\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.173401 \n",
      "\n",
      "0.3333333333333333\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.171945 \n",
      "\n",
      "0.33433433433433435\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.170527 \n",
      "\n",
      "0.3353353353353353\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.169101 \n",
      "\n",
      "0.3363363363363363\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.167693 \n",
      "\n",
      "0.33733733733733734\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.166274 \n",
      "\n",
      "0.3383383383383383\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.164897 \n",
      "\n",
      "0.33933933933933935\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.163511 \n",
      "\n",
      "0.34034034034034033\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.162119 \n",
      "\n",
      "0.34134134134134136\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.160761 \n",
      "\n",
      "0.34234234234234234\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.159400 \n",
      "\n",
      "0.3433433433433433\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.158050 \n",
      "\n",
      "0.34434434434434436\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.156696 \n",
      "\n",
      "0.34534534534534533\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.155368 \n",
      "\n",
      "0.34634634634634637\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.154058 \n",
      "\n",
      "0.34734734734734735\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.152757 \n",
      "\n",
      "0.3483483483483483\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.151446 \n",
      "\n",
      "0.34934934934934936\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.150156 \n",
      "\n",
      "0.35035035035035034\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.148859 \n",
      "\n",
      "0.35135135135135137\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.147587 \n",
      "\n",
      "0.35235235235235235\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.146312 \n",
      "\n",
      "0.3533533533533533\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.145061 \n",
      "\n",
      "0.35435435435435436\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.143819 \n",
      "\n",
      "0.35535535535535534\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.142584 \n",
      "\n",
      "0.3563563563563564\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.141356 \n",
      "\n",
      "0.35735735735735735\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.140131 \n",
      "\n",
      "0.35835835835835833\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.138930 \n",
      "\n",
      "0.35935935935935936\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.137723 \n",
      "\n",
      "0.36036036036036034\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.136542 \n",
      "\n",
      "0.3613613613613614\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.135353 \n",
      "\n",
      "0.36236236236236236\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.134175 \n",
      "\n",
      "0.36336336336336333\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.133012 \n",
      "\n",
      "0.36436436436436437\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.131857 \n",
      "\n",
      "0.36536536536536535\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.130700 \n",
      "\n",
      "0.3663663663663664\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.129577 \n",
      "\n",
      "0.36736736736736736\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.128430 \n",
      "\n",
      "0.3683683683683684\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.127314 \n",
      "\n",
      "0.36936936936936937\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.126204 \n",
      "\n",
      "0.37037037037037035\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.125094 \n",
      "\n",
      "0.3713713713713714\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.123993 \n",
      "\n",
      "0.37237237237237236\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.122921 \n",
      "\n",
      "0.3733733733733734\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.121826 \n",
      "\n",
      "0.3743743743743744\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.120754 \n",
      "\n",
      "0.37537537537537535\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.119682 \n",
      "\n",
      "0.3763763763763764\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.118631 \n",
      "\n",
      "0.37737737737737737\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.117594 \n",
      "\n",
      "0.3783783783783784\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.116565 \n",
      "\n",
      "0.3793793793793794\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.115531 \n",
      "\n",
      "0.38038038038038036\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.114515 \n",
      "\n",
      "0.3813813813813814\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.113501 \n",
      "\n",
      "0.38238238238238237\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.112482 \n",
      "\n",
      "0.3833833833833834\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.111495 \n",
      "\n",
      "0.3843843843843844\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.110498 \n",
      "\n",
      "0.38538538538538536\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.109522 \n",
      "\n",
      "0.3863863863863864\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.108548 \n",
      "\n",
      "0.38738738738738737\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.107596 \n",
      "\n",
      "0.3883883883883884\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.106633 \n",
      "\n",
      "0.3893893893893894\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.105690 \n",
      "\n",
      "0.39039039039039036\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.104738 \n",
      "\n",
      "0.3913913913913914\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.103814 \n",
      "\n",
      "0.3923923923923924\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.102881 \n",
      "\n",
      "0.3933933933933934\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.101971 \n",
      "\n",
      "0.3943943943943944\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.101052 \n",
      "\n",
      "0.39539539539539537\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.100160 \n",
      "\n",
      "0.3963963963963964\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.099265 \n",
      "\n",
      "0.3973973973973974\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.098388 \n",
      "\n",
      "0.3983983983983984\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.097497 \n",
      "\n",
      "0.3993993993993994\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.096616 \n",
      "\n",
      "0.40040040040040037\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.095753 \n",
      "\n",
      "0.4014014014014014\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.094904 \n",
      "\n",
      "0.4024024024024024\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.094059 \n",
      "\n",
      "0.4034034034034034\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.093212 \n",
      "\n",
      "0.4044044044044044\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.092374 \n",
      "\n",
      "0.40540540540540543\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.091541 \n",
      "\n",
      "0.4064064064064064\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.090734 \n",
      "\n",
      "0.4074074074074074\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.089919 \n",
      "\n",
      "0.4084084084084084\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.089109 \n",
      "\n",
      "0.4094094094094094\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.088306 \n",
      "\n",
      "0.41041041041041043\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.087522 \n",
      "\n",
      "0.4114114114114114\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.086731 \n",
      "\n",
      "0.4124124124124124\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.085958 \n",
      "\n",
      "0.4134134134134134\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.085187 \n",
      "\n",
      "0.4144144144144144\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.084422 \n",
      "\n",
      "0.41541541541541543\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.083657 \n",
      "\n",
      "0.4164164164164164\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.082907 \n",
      "\n",
      "0.4174174174174174\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.082159 \n",
      "\n",
      "0.4184184184184184\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.081428 \n",
      "\n",
      "0.4194194194194194\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.080694 \n",
      "\n",
      "0.42042042042042044\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.079972 \n",
      "\n",
      "0.4214214214214214\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.079256 \n",
      "\n",
      "0.4224224224224224\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.078536 \n",
      "\n",
      "0.42342342342342343\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.077839 \n",
      "\n",
      "0.4244244244244244\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.077138 \n",
      "\n",
      "0.42542542542542544\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.076441 \n",
      "\n",
      "0.4264264264264264\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.075754 \n",
      "\n",
      "0.4274274274274274\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.075078 \n",
      "\n",
      "0.42842842842842843\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.074403 \n",
      "\n",
      "0.4294294294294294\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.073742 \n",
      "\n",
      "0.43043043043043044\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.073082 \n",
      "\n",
      "0.4314314314314314\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.072431 \n",
      "\n",
      "0.43243243243243246\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.071779 \n",
      "\n",
      "0.43343343343343343\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.071143 \n",
      "\n",
      "0.4344344344344344\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.070490 \n",
      "\n",
      "0.43543543543543545\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.069858 \n",
      "\n",
      "0.4364364364364364\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.069236 \n",
      "\n",
      "0.43743743743743746\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.068621 \n",
      "\n",
      "0.43843843843843844\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.068009 \n",
      "\n",
      "0.4394394394394394\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.067404 \n",
      "\n",
      "0.44044044044044045\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.066796 \n",
      "\n",
      "0.44144144144144143\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.066202 \n",
      "\n",
      "0.44244244244244246\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.065611 \n",
      "\n",
      "0.44344344344344344\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.065025 \n",
      "\n",
      "0.4444444444444444\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.064452 \n",
      "\n",
      "0.44544544544544545\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.063871 \n",
      "\n",
      "0.44644644644644643\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.063305 \n",
      "\n",
      "0.44744744744744747\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.062740 \n",
      "\n",
      "0.44844844844844844\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.062183 \n",
      "\n",
      "0.4494494494494494\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.061632 \n",
      "\n",
      "0.45045045045045046\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.061089 \n",
      "\n",
      "0.45145145145145144\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.060546 \n",
      "\n",
      "0.45245245245245247\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.060001 \n",
      "\n",
      "0.45345345345345345\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.059471 \n",
      "\n",
      "0.4544544544544544\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.058947 \n",
      "\n",
      "0.45545545545545546\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.058424 \n",
      "\n",
      "0.45645645645645644\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.057912 \n",
      "\n",
      "0.4574574574574575\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.057398 \n",
      "\n",
      "0.45845845845845845\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.056890 \n",
      "\n",
      "0.45945945945945943\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.056390 \n",
      "\n",
      "0.46046046046046046\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.055894 \n",
      "\n",
      "0.46146146146146144\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.055400 \n",
      "\n",
      "0.4624624624624625\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.054916 \n",
      "\n",
      "0.46346346346346345\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.054432 \n",
      "\n",
      "0.46446446446446443\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.053953 \n",
      "\n",
      "0.46546546546546547\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.053485 \n",
      "\n",
      "0.46646646646646645\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.053012 \n",
      "\n",
      "0.4674674674674675\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.052553 \n",
      "\n",
      "0.46846846846846846\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.052092 \n",
      "\n",
      "0.4694694694694695\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.051643 \n",
      "\n",
      "0.47047047047047047\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.051183 \n",
      "\n",
      "0.47147147147147145\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.050743 \n",
      "\n",
      "0.4724724724724725\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.050305 \n",
      "\n",
      "0.47347347347347346\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.049870 \n",
      "\n",
      "0.4744744744744745\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.049438 \n",
      "\n",
      "0.4754754754754755\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.049006 \n",
      "\n",
      "0.47647647647647645\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.048576 \n",
      "\n",
      "0.4774774774774775\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.048159 \n",
      "\n",
      "0.47847847847847846\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.047743 \n",
      "\n",
      "0.4794794794794795\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.047338 \n",
      "\n",
      "0.4804804804804805\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.046926 \n",
      "\n",
      "0.48148148148148145\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.046522 \n",
      "\n",
      "0.4824824824824825\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.046121 \n",
      "\n",
      "0.48348348348348347\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.045728 \n",
      "\n",
      "0.4844844844844845\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.045338 \n",
      "\n",
      "0.4854854854854855\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.044954 \n",
      "\n",
      "0.48648648648648646\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.044575 \n",
      "\n",
      "0.4874874874874875\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.044188 \n",
      "\n",
      "0.48848848848848847\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.043815 \n",
      "\n",
      "0.4894894894894895\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.043440 \n",
      "\n",
      "0.4904904904904905\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.043083 \n",
      "\n",
      "0.49149149149149146\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.042712 \n",
      "\n",
      "0.4924924924924925\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.042354 \n",
      "\n",
      "0.4934934934934935\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.041995 \n",
      "\n",
      "0.4944944944944945\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.041639 \n",
      "\n",
      "0.4954954954954955\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.041294 \n",
      "\n",
      "0.4964964964964965\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.040944 \n",
      "\n",
      "0.4974974974974975\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.040602 \n",
      "\n",
      "0.4984984984984985\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.040262 \n",
      "\n",
      "0.4994994994994995\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.039927 \n",
      "\n",
      "0.5005005005005005\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.039595 \n",
      "\n",
      "0.5015015015015015\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.039271 \n",
      "\n",
      "0.5025025025025025\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.038948 \n",
      "\n",
      "0.5035035035035035\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.038618 \n",
      "\n",
      "0.5045045045045045\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.038297 \n",
      "\n",
      "0.5055055055055055\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.037982 \n",
      "\n",
      "0.5065065065065065\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.037672 \n",
      "\n",
      "0.5075075075075075\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.037365 \n",
      "\n",
      "0.5085085085085085\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.037059 \n",
      "\n",
      "0.5095095095095095\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.036757 \n",
      "\n",
      "0.5105105105105106\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.036461 \n",
      "\n",
      "0.5115115115115115\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.036162 \n",
      "\n",
      "0.5125125125125125\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.035867 \n",
      "\n",
      "0.5135135135135135\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.035576 \n",
      "\n",
      "0.5145145145145145\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.035284 \n",
      "\n",
      "0.5155155155155156\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.035002 \n",
      "\n",
      "0.5165165165165165\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.034722 \n",
      "\n",
      "0.5175175175175175\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.034444 \n",
      "\n",
      "0.5185185185185185\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.034166 \n",
      "\n",
      "0.5195195195195195\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.033893 \n",
      "\n",
      "0.5205205205205206\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.033625 \n",
      "\n",
      "0.5215215215215215\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.033358 \n",
      "\n",
      "0.5225225225225225\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.033092 \n",
      "\n",
      "0.5235235235235235\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.032825 \n",
      "\n",
      "0.5245245245245245\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.032567 \n",
      "\n",
      "0.5255255255255256\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.032315 \n",
      "\n",
      "0.5265265265265265\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.032061 \n",
      "\n",
      "0.5275275275275275\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.031808 \n",
      "\n",
      "0.5285285285285285\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.031559 \n",
      "\n",
      "0.5295295295295295\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.031312 \n",
      "\n",
      "0.5305305305305306\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.031072 \n",
      "\n",
      "0.5315315315315315\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.030829 \n",
      "\n",
      "0.5325325325325325\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.030591 \n",
      "\n",
      "0.5335335335335335\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.030356 \n",
      "\n",
      "0.5345345345345345\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.030120 \n",
      "\n",
      "0.5355355355355356\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.029891 \n",
      "\n",
      "0.5365365365365365\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.029660 \n",
      "\n",
      "0.5375375375375375\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.029435 \n",
      "\n",
      "0.5385385385385385\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.029212 \n",
      "\n",
      "0.5395395395395395\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.028989 \n",
      "\n",
      "0.5405405405405406\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.028766 \n",
      "\n",
      "0.5415415415415415\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.028548 \n",
      "\n",
      "0.5425425425425425\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.028336 \n",
      "\n",
      "0.5435435435435435\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.028127 \n",
      "\n",
      "0.5445445445445445\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.027915 \n",
      "\n",
      "0.5455455455455456\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.027707 \n",
      "\n",
      "0.5465465465465466\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.027505 \n",
      "\n",
      "0.5475475475475475\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.027298 \n",
      "\n",
      "0.5485485485485485\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.027090 \n",
      "\n",
      "0.5495495495495496\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.026893 \n",
      "\n",
      "0.5505505505505506\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.026694 \n",
      "\n",
      "0.5515515515515516\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.026504 \n",
      "\n",
      "0.5525525525525525\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.026313 \n",
      "\n",
      "0.5535535535535535\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.026118 \n",
      "\n",
      "0.5545545545545546\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.025929 \n",
      "\n",
      "0.5555555555555556\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.025742 \n",
      "\n",
      "0.5565565565565566\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.025556 \n",
      "\n",
      "0.5575575575575575\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.025366 \n",
      "\n",
      "0.5585585585585585\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.025187 \n",
      "\n",
      "0.5595595595595596\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.025010 \n",
      "\n",
      "0.5605605605605606\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.024829 \n",
      "\n",
      "0.5615615615615616\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.024651 \n",
      "\n",
      "0.5625625625625625\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.024480 \n",
      "\n",
      "0.5635635635635635\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.024310 \n",
      "\n",
      "0.5645645645645646\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.024138 \n",
      "\n",
      "0.5655655655655656\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.023967 \n",
      "\n",
      "0.5665665665665666\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.023794 \n",
      "\n",
      "0.5675675675675675\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.023633 \n",
      "\n",
      "0.5685685685685685\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.023469 \n",
      "\n",
      "0.5695695695695696\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.023305 \n",
      "\n",
      "0.5705705705705706\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.023148 \n",
      "\n",
      "0.5715715715715716\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.022986 \n",
      "\n",
      "0.5725725725725725\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.022833 \n",
      "\n",
      "0.5735735735735735\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.022678 \n",
      "\n",
      "0.5745745745745746\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.022518 \n",
      "\n",
      "0.5755755755755756\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.022368 \n",
      "\n",
      "0.5765765765765766\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.022219 \n",
      "\n",
      "0.5775775775775776\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.022071 \n",
      "\n",
      "0.5785785785785785\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.021921 \n",
      "\n",
      "0.5795795795795796\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.021773 \n",
      "\n",
      "0.5805805805805806\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.021627 \n",
      "\n",
      "0.5815815815815816\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.021488 \n",
      "\n",
      "0.5825825825825826\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.021346 \n",
      "\n",
      "0.5835835835835835\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.021203 \n",
      "\n",
      "0.5845845845845846\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.021065 \n",
      "\n",
      "0.5855855855855856\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.020926 \n",
      "\n",
      "0.5865865865865866\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.020790 \n",
      "\n",
      "0.5875875875875876\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.020655 \n",
      "\n",
      "0.5885885885885885\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.020526 \n",
      "\n",
      "0.5895895895895896\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.020391 \n",
      "\n",
      "0.5905905905905906\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.020256 \n",
      "\n",
      "0.5915915915915916\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.020132 \n",
      "\n",
      "0.5925925925925926\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.020001 \n",
      "\n",
      "0.5935935935935935\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.019875 \n",
      "\n",
      "0.5945945945945946\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.019751 \n",
      "\n",
      "0.5955955955955956\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.019625 \n",
      "\n",
      "0.5965965965965966\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.019501 \n",
      "\n",
      "0.5975975975975976\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.019381 \n",
      "\n",
      "0.5985985985985985\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.019261 \n",
      "\n",
      "0.5995995995995996\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.019142 \n",
      "\n",
      "0.6006006006006006\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.019025 \n",
      "\n",
      "0.6016016016016016\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.018905 \n",
      "\n",
      "0.6026026026026026\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.018788 \n",
      "\n",
      "0.6036036036036035\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.018673 \n",
      "\n",
      "0.6046046046046046\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.018561 \n",
      "\n",
      "0.6056056056056056\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.018452 \n",
      "\n",
      "0.6066066066066066\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.018339 \n",
      "\n",
      "0.6076076076076076\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.018230 \n",
      "\n",
      "0.6086086086086087\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.018120 \n",
      "\n",
      "0.6096096096096096\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.018012 \n",
      "\n",
      "0.6106106106106106\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.017906 \n",
      "\n",
      "0.6116116116116116\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.017798 \n",
      "\n",
      "0.6126126126126126\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.017695 \n",
      "\n",
      "0.6136136136136137\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.017590 \n",
      "\n",
      "0.6146146146146146\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.017491 \n",
      "\n",
      "0.6156156156156156\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.017389 \n",
      "\n",
      "0.6166166166166166\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.017288 \n",
      "\n",
      "0.6176176176176176\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.017187 \n",
      "\n",
      "0.6186186186186187\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.017091 \n",
      "\n",
      "0.6196196196196196\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.016992 \n",
      "\n",
      "0.6206206206206206\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.016895 \n",
      "\n",
      "0.6216216216216216\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.016798 \n",
      "\n",
      "0.6226226226226226\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.016706 \n",
      "\n",
      "0.6236236236236237\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.016610 \n",
      "\n",
      "0.6246246246246246\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.016519 \n",
      "\n",
      "0.6256256256256256\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.016427 \n",
      "\n",
      "0.6266266266266266\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.016335 \n",
      "\n",
      "0.6276276276276276\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.016242 \n",
      "\n",
      "0.6286286286286287\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.016154 \n",
      "\n",
      "0.6296296296296297\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.016066 \n",
      "\n",
      "0.6306306306306306\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.015979 \n",
      "\n",
      "0.6316316316316316\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.015892 \n",
      "\n",
      "0.6326326326326326\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.015802 \n",
      "\n",
      "0.6336336336336337\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.015718 \n",
      "\n",
      "0.6346346346346347\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.015635 \n",
      "\n",
      "0.6356356356356356\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.015551 \n",
      "\n",
      "0.6366366366366366\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.015469 \n",
      "\n",
      "0.6376376376376376\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.015384 \n",
      "\n",
      "0.6386386386386387\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.015303 \n",
      "\n",
      "0.6396396396396397\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.015224 \n",
      "\n",
      "0.6406406406406406\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.015143 \n",
      "\n",
      "0.6416416416416416\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.015064 \n",
      "\n",
      "0.6426426426426426\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.014989 \n",
      "\n",
      "0.6436436436436437\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.014910 \n",
      "\n",
      "0.6446446446446447\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.014835 \n",
      "\n",
      "0.6456456456456456\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.014758 \n",
      "\n",
      "0.6466466466466466\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.014680 \n",
      "\n",
      "0.6476476476476476\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.014606 \n",
      "\n",
      "0.6486486486486487\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.014535 \n",
      "\n",
      "0.6496496496496497\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.014460 \n",
      "\n",
      "0.6506506506506506\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.014387 \n",
      "\n",
      "0.6516516516516516\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.014314 \n",
      "\n",
      "0.6526526526526526\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.014245 \n",
      "\n",
      "0.6536536536536537\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.014173 \n",
      "\n",
      "0.6546546546546547\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.014105 \n",
      "\n",
      "0.6556556556556556\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.014033 \n",
      "\n",
      "0.6566566566566566\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.013966 \n",
      "\n",
      "0.6576576576576576\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.013898 \n",
      "\n",
      "0.6586586586586587\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.013830 \n",
      "\n",
      "0.6596596596596597\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.013765 \n",
      "\n",
      "0.6606606606606606\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.013699 \n",
      "\n",
      "0.6616616616616616\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.013633 \n",
      "\n",
      "0.6626626626626626\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.013565 \n",
      "\n",
      "0.6636636636636637\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.013502 \n",
      "\n",
      "0.6646646646646647\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.013438 \n",
      "\n",
      "0.6656656656656657\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.013374 \n",
      "\n",
      "0.6666666666666666\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.013310 \n",
      "\n",
      "0.6676676676676676\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.013248 \n",
      "\n",
      "0.6686686686686687\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.013187 \n",
      "\n",
      "0.6696696696696697\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.013128 \n",
      "\n",
      "0.6706706706706707\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.013067 \n",
      "\n",
      "0.6716716716716716\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.013007 \n",
      "\n",
      "0.6726726726726726\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.012946 \n",
      "\n",
      "0.6736736736736737\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.012887 \n",
      "\n",
      "0.6746746746746747\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.012827 \n",
      "\n",
      "0.6756756756756757\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.012771 \n",
      "\n",
      "0.6766766766766766\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.012713 \n",
      "\n",
      "0.6776776776776777\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.012659 \n",
      "\n",
      "0.6786786786786787\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.012601 \n",
      "\n",
      "0.6796796796796797\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.012543 \n",
      "\n",
      "0.6806806806806807\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.012488 \n",
      "\n",
      "0.6816816816816816\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.012431 \n",
      "\n",
      "0.6826826826826827\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.012378 \n",
      "\n",
      "0.6836836836836837\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.012325 \n",
      "\n",
      "0.6846846846846847\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.012269 \n",
      "\n",
      "0.6856856856856857\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.012217 \n",
      "\n",
      "0.6866866866866866\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.012164 \n",
      "\n",
      "0.6876876876876877\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.012110 \n",
      "\n",
      "0.6886886886886887\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.012059 \n",
      "\n",
      "0.6896896896896897\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.012008 \n",
      "\n",
      "0.6906906906906907\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011956 \n",
      "\n",
      "0.6916916916916916\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011905 \n",
      "\n",
      "0.6926926926926927\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011854 \n",
      "\n",
      "0.6936936936936937\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011803 \n",
      "\n",
      "0.6946946946946947\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011756 \n",
      "\n",
      "0.6956956956956957\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011705 \n",
      "\n",
      "0.6966966966966966\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011658 \n",
      "\n",
      "0.6976976976976977\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011610 \n",
      "\n",
      "0.6986986986986987\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011561 \n",
      "\n",
      "0.6996996996996997\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011513 \n",
      "\n",
      "0.7007007007007007\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011466 \n",
      "\n",
      "0.7017017017017017\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011420 \n",
      "\n",
      "0.7027027027027027\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011372 \n",
      "\n",
      "0.7037037037037037\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011327 \n",
      "\n",
      "0.7047047047047047\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011281 \n",
      "\n",
      "0.7057057057057057\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011235 \n",
      "\n",
      "0.7067067067067067\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011191 \n",
      "\n",
      "0.7077077077077077\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011146 \n",
      "\n",
      "0.7087087087087087\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011100 \n",
      "\n",
      "0.7097097097097097\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011056 \n",
      "\n",
      "0.7107107107107107\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.011012 \n",
      "\n",
      "0.7117117117117117\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.010970 \n",
      "\n",
      "0.7127127127127127\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.010926 \n",
      "\n",
      "0.7137137137137137\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.010884 \n",
      "\n",
      "0.7147147147147147\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.010842 \n",
      "\n",
      "0.7157157157157157\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.010801 \n",
      "\n",
      "0.7167167167167167\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.010758 \n",
      "\n",
      "0.7177177177177178\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.010716 \n",
      "\n",
      "0.7187187187187187\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.010674 \n",
      "\n",
      "0.7197197197197197\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.010634 \n",
      "\n",
      "0.7207207207207207\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.010593 \n",
      "\n",
      "0.7217217217217217\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.010553 \n",
      "\n",
      "0.7227227227227228\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.010512 \n",
      "\n",
      "0.7237237237237237\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.010472 \n",
      "\n",
      "0.7247247247247247\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.010435 \n",
      "\n",
      "0.7257257257257257\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.010394 \n",
      "\n",
      "0.7267267267267267\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.010355 \n",
      "\n",
      "0.7277277277277278\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.010317 \n",
      "\n",
      "0.7287287287287287\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.010277 \n",
      "\n",
      "0.7297297297297297\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.010240 \n",
      "\n",
      "0.7307307307307307\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 0.010202 \n",
      "\n",
      "0.7317317317317317\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 0.010164 \n",
      "\n",
      "0.7327327327327328\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.010128 \n",
      "\n",
      "0.7337337337337337\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.010092 \n",
      "\n",
      "0.7347347347347347\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.010054 \n",
      "\n",
      "0.7357357357357357\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.010018 \n",
      "\n",
      "0.7367367367367368\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.009981 \n",
      "\n",
      "0.7377377377377378\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.009944 \n",
      "\n",
      "0.7387387387387387\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.009908 \n",
      "\n",
      "0.7397397397397397\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.009873 \n",
      "\n",
      "0.7407407407407407\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009838 \n",
      "\n",
      "0.7417417417417418\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009803 \n",
      "\n",
      "0.7427427427427428\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009767 \n",
      "\n",
      "0.7437437437437437\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009733 \n",
      "\n",
      "0.7447447447447447\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009698 \n",
      "\n",
      "0.7457457457457457\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009664 \n",
      "\n",
      "0.7467467467467468\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009629 \n",
      "\n",
      "0.7477477477477478\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009594 \n",
      "\n",
      "0.7487487487487487\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009562 \n",
      "\n",
      "0.7497497497497497\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009528 \n",
      "\n",
      "0.7507507507507507\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009494 \n",
      "\n",
      "0.7517517517517518\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009461 \n",
      "\n",
      "0.7527527527527528\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009429 \n",
      "\n",
      "0.7537537537537538\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009395 \n",
      "\n",
      "0.7547547547547547\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009364 \n",
      "\n",
      "0.7557557557557557\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009330 \n",
      "\n",
      "0.7567567567567568\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009299 \n",
      "\n",
      "0.7577577577577578\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009268 \n",
      "\n",
      "0.7587587587587588\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009235 \n",
      "\n",
      "0.7597597597597597\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009203 \n",
      "\n",
      "0.7607607607607607\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009173 \n",
      "\n",
      "0.7617617617617618\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009142 \n",
      "\n",
      "0.7627627627627628\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009110 \n",
      "\n",
      "0.7637637637637638\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009078 \n",
      "\n",
      "0.7647647647647647\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009049 \n",
      "\n",
      "0.7657657657657657\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.009019 \n",
      "\n",
      "0.7667667667667668\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.008989 \n",
      "\n",
      "0.7677677677677678\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.008958 \n",
      "\n",
      "0.7687687687687688\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.008929 \n",
      "\n",
      "0.7697697697697697\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.008898 \n",
      "\n",
      "0.7707707707707707\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.008869 \n",
      "\n",
      "0.7717717717717718\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.008838 \n",
      "\n",
      "0.7727727727727728\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.008810 \n",
      "\n",
      "0.7737737737737738\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.008781 \n",
      "\n",
      "0.7747747747747747\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.008752 \n",
      "\n",
      "0.7757757757757757\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.008723 \n",
      "\n",
      "0.7767767767767768\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.008693 \n",
      "\n",
      "0.7777777777777778\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.008665 \n",
      "\n",
      "0.7787787787787788\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.008637 \n",
      "\n",
      "0.7797797797797797\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.008610 \n",
      "\n",
      "0.7807807807807807\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.008581 \n",
      "\n",
      "0.7817817817817818\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.008553 \n",
      "\n",
      "0.7827827827827828\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.008526 \n",
      "\n",
      "0.7837837837837838\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.008498 \n",
      "\n",
      "0.7847847847847848\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.008469 \n",
      "\n",
      "0.7857857857857857\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.008443 \n",
      "\n",
      "0.7867867867867868\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.008416 \n",
      "\n",
      "0.7877877877877878\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.008388 \n",
      "\n",
      "0.7887887887887888\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.008361 \n",
      "\n",
      "0.7897897897897898\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.6%, Avg loss: 0.008334 \n",
      "\n",
      "0.7907907907907907\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.008308 \n",
      "\n",
      "0.7917917917917918\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.008281 \n",
      "\n",
      "0.7927927927927928\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.3%, Avg loss: 0.008254 \n",
      "\n",
      "0.7937937937937938\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.008228 \n",
      "\n",
      "0.7947947947947948\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.008203 \n",
      "\n",
      "0.7957957957957957\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.008176 \n",
      "\n",
      "0.7967967967967968\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.1%, Avg loss: 0.008150 \n",
      "\n",
      "0.7977977977977978\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.008124 \n",
      "\n",
      "0.7987987987987988\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.008098 \n",
      "\n",
      "0.7997997997997998\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.008073 \n",
      "\n",
      "0.8008008008008007\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.008047 \n",
      "\n",
      "0.8018018018018018\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.008022 \n",
      "\n",
      "0.8028028028028028\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.9%, Avg loss: 0.007996 \n",
      "\n",
      "0.8038038038038038\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.9%, Avg loss: 0.007971 \n",
      "\n",
      "0.8048048048048048\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.9%, Avg loss: 0.007945 \n",
      "\n",
      "0.8058058058058059\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.9%, Avg loss: 0.007921 \n",
      "\n",
      "0.8068068068068068\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.007896 \n",
      "\n",
      "0.8078078078078078\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.007871 \n",
      "\n",
      "0.8088088088088088\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.007845 \n",
      "\n",
      "0.8098098098098098\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.007822 \n",
      "\n",
      "0.8108108108108109\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.007797 \n",
      "\n",
      "0.8118118118118118\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.007773 \n",
      "\n",
      "0.8128128128128128\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.007748 \n",
      "\n",
      "0.8138138138138138\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.007724 \n",
      "\n",
      "0.8148148148148148\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.007701 \n",
      "\n",
      "0.8158158158158159\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.6%, Avg loss: 0.007677 \n",
      "\n",
      "0.8168168168168168\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.6%, Avg loss: 0.007653 \n",
      "\n",
      "0.8178178178178178\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.4%, Avg loss: 0.007630 \n",
      "\n",
      "0.8188188188188188\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.3%, Avg loss: 0.007606 \n",
      "\n",
      "0.8198198198198198\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.0%, Avg loss: 0.007583 \n",
      "\n",
      "0.8208208208208209\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.2%, Avg loss: 0.007559 \n",
      "\n",
      "0.8218218218218218\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.0%, Avg loss: 0.007535 \n",
      "\n",
      "0.8228228228228228\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 96.0%, Avg loss: 0.007512 \n",
      "\n",
      "0.8238238238238238\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 95.9%, Avg loss: 0.007489 \n",
      "\n",
      "0.8248248248248248\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 95.9%, Avg loss: 0.007466 \n",
      "\n",
      "0.8258258258258259\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 95.8%, Avg loss: 0.007443 \n",
      "\n",
      "0.8268268268268268\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 95.2%, Avg loss: 0.007420 \n",
      "\n",
      "0.8278278278278278\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.8%, Avg loss: 0.007398 \n",
      "\n",
      "0.8288288288288288\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.8%, Avg loss: 0.007375 \n",
      "\n",
      "0.8298298298298298\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.8%, Avg loss: 0.007352 \n",
      "\n",
      "0.8308308308308309\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.8%, Avg loss: 0.007329 \n",
      "\n",
      "0.8318318318318318\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.8%, Avg loss: 0.007307 \n",
      "\n",
      "0.8328328328328328\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.7%, Avg loss: 0.007284 \n",
      "\n",
      "0.8338338338338338\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.7%, Avg loss: 0.007262 \n",
      "\n",
      "0.8348348348348348\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.7%, Avg loss: 0.007240 \n",
      "\n",
      "0.8358358358358359\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.7%, Avg loss: 0.007218 \n",
      "\n",
      "0.8368368368368369\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.7%, Avg loss: 0.007197 \n",
      "\n",
      "0.8378378378378378\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.7%, Avg loss: 0.007174 \n",
      "\n",
      "0.8388388388388388\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.007152 \n",
      "\n",
      "0.8398398398398398\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.007131 \n",
      "\n",
      "0.8408408408408409\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.007108 \n",
      "\n",
      "0.8418418418418419\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.007087 \n",
      "\n",
      "0.8428428428428428\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.007066 \n",
      "\n",
      "0.8438438438438438\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.007044 \n",
      "\n",
      "0.8448448448448448\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.007023 \n",
      "\n",
      "0.8458458458458459\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.007001 \n",
      "\n",
      "0.8468468468468469\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.006980 \n",
      "\n",
      "0.8478478478478478\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.006959 \n",
      "\n",
      "0.8488488488488488\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.006937 \n",
      "\n",
      "0.8498498498498498\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.006917 \n",
      "\n",
      "0.8508508508508509\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.006896 \n",
      "\n",
      "0.8518518518518519\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.6%, Avg loss: 0.006875 \n",
      "\n",
      "0.8528528528528528\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.5%, Avg loss: 0.006854 \n",
      "\n",
      "0.8538538538538538\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.5%, Avg loss: 0.006834 \n",
      "\n",
      "0.8548548548548548\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.5%, Avg loss: 0.006813 \n",
      "\n",
      "0.8558558558558559\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.5%, Avg loss: 0.006793 \n",
      "\n",
      "0.8568568568568569\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.5%, Avg loss: 0.006772 \n",
      "\n",
      "0.8578578578578578\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.5%, Avg loss: 0.006753 \n",
      "\n",
      "0.8588588588588588\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.5%, Avg loss: 0.006731 \n",
      "\n",
      "0.8598598598598598\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.4%, Avg loss: 0.006711 \n",
      "\n",
      "0.8608608608608609\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.4%, Avg loss: 0.006691 \n",
      "\n",
      "0.8618618618618619\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.4%, Avg loss: 0.006671 \n",
      "\n",
      "0.8628628628628628\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.4%, Avg loss: 0.006651 \n",
      "\n",
      "0.8638638638638638\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.2%, Avg loss: 0.006630 \n",
      "\n",
      "0.8648648648648649\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.2%, Avg loss: 0.006610 \n",
      "\n",
      "0.8658658658658659\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.1%, Avg loss: 0.006590 \n",
      "\n",
      "0.8668668668668669\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.1%, Avg loss: 0.006571 \n",
      "\n",
      "0.8678678678678678\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.1%, Avg loss: 0.006551 \n",
      "\n",
      "0.8688688688688688\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.2%, Avg loss: 0.006531 \n",
      "\n",
      "0.8698698698698699\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.1%, Avg loss: 0.006512 \n",
      "\n",
      "0.8708708708708709\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.1%, Avg loss: 0.006493 \n",
      "\n",
      "0.8718718718718719\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 94.0%, Avg loss: 0.006474 \n",
      "\n",
      "0.8728728728728729\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.9%, Avg loss: 0.006453 \n",
      "\n",
      "0.8738738738738738\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.006434 \n",
      "\n",
      "0.8748748748748749\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.006415 \n",
      "\n",
      "0.8758758758758759\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.7%, Avg loss: 0.006396 \n",
      "\n",
      "0.8768768768768769\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.7%, Avg loss: 0.006376 \n",
      "\n",
      "0.8778778778778779\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.006357 \n",
      "\n",
      "0.8788788788788788\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.006338 \n",
      "\n",
      "0.8798798798798799\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.006321 \n",
      "\n",
      "0.8808808808808809\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.006300 \n",
      "\n",
      "0.8818818818818819\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.006283 \n",
      "\n",
      "0.8828828828828829\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.6%, Avg loss: 0.006263 \n",
      "\n",
      "0.8838838838838838\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.5%, Avg loss: 0.006244 \n",
      "\n",
      "0.8848848848848849\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.5%, Avg loss: 0.006226 \n",
      "\n",
      "0.8858858858858859\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.006207 \n",
      "\n",
      "0.8868868868868869\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.006189 \n",
      "\n",
      "0.8878878878878879\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.006170 \n",
      "\n",
      "0.8888888888888888\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.006152 \n",
      "\n",
      "0.8898898898898899\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.006134 \n",
      "\n",
      "0.8908908908908909\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.006115 \n",
      "\n",
      "0.8918918918918919\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.006097 \n",
      "\n",
      "0.8928928928928929\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.006080 \n",
      "\n",
      "0.8938938938938938\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.0%, Avg loss: 0.006062 \n",
      "\n",
      "0.8948948948948949\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.0%, Avg loss: 0.006043 \n",
      "\n",
      "0.8958958958958959\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 93.0%, Avg loss: 0.006026 \n",
      "\n",
      "0.8968968968968969\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.006008 \n",
      "\n",
      "0.8978978978978979\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.005990 \n",
      "\n",
      "0.8988988988988988\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.8%, Avg loss: 0.005972 \n",
      "\n",
      "0.8998998998998999\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.8%, Avg loss: 0.005955 \n",
      "\n",
      "0.9009009009009009\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.005938 \n",
      "\n",
      "0.9019019019019019\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.005919 \n",
      "\n",
      "0.9029029029029029\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.6%, Avg loss: 0.005901 \n",
      "\n",
      "0.9039039039039038\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.6%, Avg loss: 0.005884 \n",
      "\n",
      "0.9049049049049049\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.6%, Avg loss: 0.005867 \n",
      "\n",
      "0.9059059059059059\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.005850 \n",
      "\n",
      "0.9069069069069069\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.005832 \n",
      "\n",
      "0.9079079079079079\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.005815 \n",
      "\n",
      "0.9089089089089089\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.005798 \n",
      "\n",
      "0.9099099099099099\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.005781 \n",
      "\n",
      "0.9109109109109109\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.4%, Avg loss: 0.005764 \n",
      "\n",
      "0.9119119119119119\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.005747 \n",
      "\n",
      "0.9129129129129129\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.005730 \n",
      "\n",
      "0.9139139139139139\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 92.0%, Avg loss: 0.005715 \n",
      "\n",
      "0.914914914914915\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.005696 \n",
      "\n",
      "0.9159159159159159\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.005680 \n",
      "\n",
      "0.9169169169169169\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.005663 \n",
      "\n",
      "0.9179179179179179\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.005647 \n",
      "\n",
      "0.9189189189189189\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.005631 \n",
      "\n",
      "0.91991991991992\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.005613 \n",
      "\n",
      "0.9209209209209209\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.005598 \n",
      "\n",
      "0.9219219219219219\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.005581 \n",
      "\n",
      "0.9229229229229229\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 91.3%, Avg loss: 0.005565 \n",
      "\n",
      "0.9239239239239239\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.005549 \n",
      "\n",
      "0.924924924924925\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.005533 \n",
      "\n",
      "0.9259259259259259\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 91.1%, Avg loss: 0.005516 \n",
      "\n",
      "0.9269269269269269\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.005501 \n",
      "\n",
      "0.9279279279279279\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.005485 \n",
      "\n",
      "0.9289289289289289\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.005469 \n",
      "\n",
      "0.92992992992993\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.005453 \n",
      "\n",
      "0.9309309309309309\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.005437 \n",
      "\n",
      "0.9319319319319319\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 90.6%, Avg loss: 0.005422 \n",
      "\n",
      "0.9329329329329329\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.005406 \n",
      "\n",
      "0.933933933933934\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.005391 \n",
      "\n",
      "0.934934934934935\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.005375 \n",
      "\n",
      "0.9359359359359359\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.005359 \n",
      "\n",
      "0.9369369369369369\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.005343 \n",
      "\n",
      "0.9379379379379379\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.005329 \n",
      "\n",
      "0.938938938938939\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 90.1%, Avg loss: 0.005315 \n",
      "\n",
      "0.93993993993994\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 89.8%, Avg loss: 0.005298 \n",
      "\n",
      "0.9409409409409409\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 89.8%, Avg loss: 0.005284 \n",
      "\n",
      "0.9419419419419419\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.005268 \n",
      "\n",
      "0.9429429429429429\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.005254 \n",
      "\n",
      "0.943943943943944\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.005238 \n",
      "\n",
      "0.944944944944945\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 89.6%, Avg loss: 0.005224 \n",
      "\n",
      "0.9459459459459459\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 89.5%, Avg loss: 0.005209 \n",
      "\n",
      "0.9469469469469469\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.005195 \n",
      "\n",
      "0.9479479479479479\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.005180 \n",
      "\n",
      "0.948948948948949\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 89.2%, Avg loss: 0.005165 \n",
      "\n",
      "0.94994994994995\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.005151 \n",
      "\n",
      "0.950950950950951\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 89.0%, Avg loss: 0.005137 \n",
      "\n",
      "0.9519519519519519\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.005122 \n",
      "\n",
      "0.9529529529529529\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.005107 \n",
      "\n",
      "0.953953953953954\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.005094 \n",
      "\n",
      "0.954954954954955\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.005078 \n",
      "\n",
      "0.955955955955956\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.005065 \n",
      "\n",
      "0.9569569569569569\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.005051 \n",
      "\n",
      "0.9579579579579579\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.005037 \n",
      "\n",
      "0.958958958958959\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.005022 \n",
      "\n",
      "0.95995995995996\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.7%, Avg loss: 0.005009 \n",
      "\n",
      "0.960960960960961\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.004995 \n",
      "\n",
      "0.9619619619619619\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.004982 \n",
      "\n",
      "0.9629629629629629\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.5%, Avg loss: 0.004968 \n",
      "\n",
      "0.963963963963964\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.4%, Avg loss: 0.004955 \n",
      "\n",
      "0.964964964964965\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.004941 \n",
      "\n",
      "0.965965965965966\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.004928 \n",
      "\n",
      "0.9669669669669669\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.004915 \n",
      "\n",
      "0.9679679679679679\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 88.1%, Avg loss: 0.004901 \n",
      "\n",
      "0.968968968968969\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 87.9%, Avg loss: 0.004888 \n",
      "\n",
      "0.96996996996997\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.004876 \n",
      "\n",
      "0.970970970970971\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.004862 \n",
      "\n",
      "0.9719719719719719\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 87.7%, Avg loss: 0.004849 \n",
      "\n",
      "0.9729729729729729\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.004836 \n",
      "\n",
      "0.973973973973974\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.004823 \n",
      "\n",
      "0.974974974974975\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.004811 \n",
      "\n",
      "0.975975975975976\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.004798 \n",
      "\n",
      "0.9769769769769769\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.004785 \n",
      "\n",
      "0.9779779779779779\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.004772 \n",
      "\n",
      "0.978978978978979\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.004760 \n",
      "\n",
      "0.97997997997998\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.004747 \n",
      "\n",
      "0.980980980980981\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.004736 \n",
      "\n",
      "0.9819819819819819\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 87.1%, Avg loss: 0.004722 \n",
      "\n",
      "0.9829829829829829\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 86.9%, Avg loss: 0.004710 \n",
      "\n",
      "0.983983983983984\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 86.8%, Avg loss: 0.004699 \n",
      "\n",
      "0.984984984984985\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.004687 \n",
      "\n",
      "0.985985985985986\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.004675 \n",
      "\n",
      "0.986986986986987\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 86.6%, Avg loss: 0.004663 \n",
      "\n",
      "0.9879879879879879\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.004650 \n",
      "\n",
      "0.988988988988989\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.004638 \n",
      "\n",
      "0.98998998998999\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.004627 \n",
      "\n",
      "0.990990990990991\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.004615 \n",
      "\n",
      "0.991991991991992\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 86.2%, Avg loss: 0.004603 \n",
      "\n",
      "0.992992992992993\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.004592 \n",
      "\n",
      "0.993993993993994\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.004581 \n",
      "\n",
      "0.994994994994995\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.004570 \n",
      "\n",
      "0.995995995995996\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 86.3%, Avg loss: 0.004558 \n",
      "\n",
      "0.996996996996997\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.004547 \n",
      "\n",
      "0.997997997997998\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.004536 \n",
      "\n",
      "0.998998998998999\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.004525 \n",
      "\n",
      "1.0\n",
      "2500\n",
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.004514 \n",
      "\n",
      "0.9896\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "best_score=0\n",
    "best_t=-1\n",
    "\n",
    "for t in np.linspace(0,1,1000):\n",
    "    print(t)\n",
    "    od = linear_combo_dic(model_1,model_5,t)\n",
    "    test_model.load_state_dict(od)\n",
    "    \n",
    "    score = test(test_dataloader, test_model, loss_fn)\n",
    "    \n",
    "    if best_score <score:\n",
    "        best_score=score\n",
    "        best_t=t\n",
    "        \n",
    "\n",
    "print(best_score)\n",
    "print(best_t)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbc79d6-694f-416f-9a88-69249ef71eb5",
   "metadata": {},
   "source": [
    "So there you have it.  \n",
    "\n",
    "It's important to note that a 99% accuracy rate is suspect is likely already overfitting. \n",
    "But it's just to say that it's still possible to make improvements via just checking around the area manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac2efc6-a5ab-4df3-8996-62b9bcdad2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
